# main.py:
import os
import json
import sys
import time
import traceback

import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from google.genai import errors
from streamlit_ace import st_ace

# --- Local Module Imports ---
try:
    from gp_chat import config
    from gp_chat import utils
    from gp_chat import sidebar
except ImportError:
    import config
    import utils
    import sidebar

# --- Helper Functions ---

def add_debug_log(message, level="info"):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«è¨˜éŒ²ã—ã¾ã™ã€‚"""
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []
    
    timestamp = time.strftime("%H:%M:%S")
    st.session_state["debug_logs"].append(f"[{timestamp}] [{level.upper()}] {message}")
    if len(st.session_state["debug_logs"]) > 50:
        st.session_state["debug_logs"].pop(0)

def load_history(uploader_key):
    """JSONã‹ã‚‰ä¼šè©±å±¥æ­´ã¨Canvasã‚’å¾©å…ƒã—ã¾ã™ã€‚"""
    uploaded_file = st.session_state.get(uploader_key)
    if not uploaded_file:
        return
    try:
        loaded_data = json.load(uploaded_file)
        if isinstance(loaded_data, dict) and "messages" in loaded_data:
            st.session_state['messages'] = loaded_data["messages"]
            if "python_canvases" in loaded_data:
                st.session_state['python_canvases'] = loaded_data["python_canvases"]
            
            if "multi_code_enabled" in loaded_data:
                st.session_state['multi_code_enabled'] = loaded_data["multi_code_enabled"]

            st.success(config.UITexts.HISTORY_LOADED_SUCCESS)
            st.session_state['system_role_defined'] = True
            st.session_state['canvas_key_counter'] += 1
            add_debug_log("Session restored from JSON.")

    except Exception as e:
        st.error(f"Load failed: {e}")
        add_debug_log(f"Restore error: {e}", "error")

def recover_interrupted_session():
    """
    ä¸­æ–­ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã§çµ‚ã‚ã£ã¦ã„ã‚‹çŠ¶æ…‹ï¼‰ã‚’æ¤œçŸ¥ã—ã€
    å±¥æ­´ã‹ã‚‰å‰Šé™¤ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‰ãƒ©ãƒ•ãƒˆé ˜åŸŸã«å¾©å…ƒã—ã¾ã™ã€‚
    """
    messages = st.session_state.get('messages', [])
    
    # æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ã€ã‹ã¤ç”Ÿæˆä¸­ãƒ•ãƒ©ã‚°ãŒç«‹ã£ã¦ã„ãªã„ï¼ˆã¾ãŸã¯ä¸­æ–­å¾Œã®ãƒªãƒ©ãƒ³ï¼‰å ´åˆ
    # ãŸã ã—ã€systemãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã ã‘ã®æ™‚ã¯é™¤å¤–
    if messages and messages[-1]["role"] == "user":
        # ã“ã“ã§ã€ŒAIã®å¿œç­”å¾…ã¡ã€ã®çŠ¶æ…‹ã§ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã§ã™ãŒã€
        # Streamlitã®ãƒ•ãƒ­ãƒ¼ä¸Šã€'is_generating' ãŒ True ã®ã¾ã¾ä¸­æ–­ã•ã‚Œã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚
        # ã—ãŸãŒã£ã¦ã€ã€Œèµ·å‹•æ™‚ã«æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã€ï¼ã€Œå¿œç­”ãŒå®Œäº†ã—ã¦ã„ãªã„ã€ã¨ã¿ãªã—ã¾ã™ã€‚
        
        last_user_msg = messages.pop() # å±¥æ­´ã‹ã‚‰å‰Šé™¤
        content = last_user_msg["content"]
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜ãŒã‚ã£ãŸå ´åˆã®å‡¦ç†ï¼ˆç°¡æ˜“çš„ã«ãƒ†ã‚­ã‚¹ãƒˆã ã‘å¾©å…ƒï¼‰
        # â€»æœ¬æ¥ã¯æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å¾©å…ƒã™ã¹ãã§ã™ãŒã€ä»Šå›ã¯ãƒ†ã‚­ã‚¹ãƒˆå¾©å…ƒã‚’å„ªå…ˆã—ã¾ã™
        
        st.session_state['draft_input'] = content
        st.session_state['is_generating'] = False # ç”Ÿæˆãƒ•ãƒ©ã‚°ã‚’ãƒªã‚»ãƒƒãƒˆ
        
        add_debug_log("Detected interrupted session. Restored draft text.")
        return True
    return False

# --- Streamlit Application ---

def run_chatbot_app():
    st.set_page_config(page_title=config.UITexts.APP_TITLE, layout="wide")
    st.title(config.UITexts.APP_TITLE)
    
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»
    PROMPTS = utils.load_prompts()
    APP_CONFIG = utils.load_app_config()
    supported_extensions = APP_CONFIG.get("file_uploader", {}).get("supported_extensions", [])
    env_files = utils.find_env_files()
    
    if not env_files:
        st.error("env ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        st.stop()

    for key, value in config.SESSION_STATE_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value.copy() if isinstance(value, (dict, list)) else value

    # --- æ©Ÿèƒ½æ”¹å–„: ä¸­æ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒã‚§ãƒƒã‚¯ ---
    # ã‚¢ãƒ—ãƒªã®ãƒªãƒ©ãƒ³æ™‚ï¼ˆåœæ­¢ãƒœã‚¿ãƒ³æŠ¼ä¸‹å¾Œãªã©ï¼‰ã«ã€å®Œäº†ã—ã¦ã„ãªã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Œã°å¾©å…ƒ
    # ãŸã ã—ã€é€šå¸¸ã®ã€Œé€ä¿¡ç›´å¾Œï¼ˆis_generating=Trueã«ãªã‚ŠãŸã¦ï¼‰ã€ã¯é™¤å¤–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
    # ã“ã“ã§ã¯ã€ã€Œé€ä¿¡ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ãŸç›´å¾Œã€ã‚’åŒºåˆ¥ã™ã‚‹ã®ãŒé›£ã—ã„ãŸã‚ã€
    # ã‚·ãƒ³ãƒ—ãƒ«ã«ã€Œç”Ÿæˆå‡¦ç†ãƒ–ãƒ­ãƒƒã‚¯ã«å…¥ã‚‰ãšã«ã“ã“ã«æ¥ãŸï¼ä¸­æ–­ã•ã‚ŒãŸã€ã¨åˆ¤æ–­ã—ã¾ã™ã€‚
    # å®Ÿéš›ã«ã¯ generate ãƒ­ã‚¸ãƒƒã‚¯ã®å¾Œã§ãƒ•ãƒ©ã‚°ã‚’è½ã¨ã™ã®ã§ã€æ¬¡å›èµ·å‹•æ™‚ã«ãƒ•ãƒ©ã‚°ãŒæ®‹ã£ã¦ã„ã‚‹ or LastãŒUserãªã‚‰ä¸­æ–­ã§ã™ã€‚
    
    # Session Stateã« 'draft_input' ãŒãªã„å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
    if 'draft_input' not in st.session_state:
        # ç›´å‰ã®å®Ÿè¡ŒãŒ generate å‡¦ç†ã¾ã§åˆ°é”ã›ãšã«çµ‚äº†ã—ãŸå ´åˆã®æ¤œçŸ¥ã¯é›£ã—ã„ã§ã™ãŒã€
        # ã€Œåœæ­¢ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒåœæ­¢ã—ã€æ¬¡å›ãƒªãƒ­ãƒ¼ãƒ‰æ™‚ã«å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
        pass

    sidebar.render_sidebar(
        supported_extensions, env_files, load_history, 
        lambda i: st.session_state['python_canvases'].__setitem__(i, config.ACE_EDITOR_DEFAULT_CODE),
        lambda i, m: (st.session_state['messages'].append({"role": "user", "content": config.UITexts.REVIEW_PROMPT_MULTI.format(i=i+1) if m else config.UITexts.REVIEW_PROMPT_SINGLE}), st.session_state.__setitem__('is_generating', True)),
        lambda i: utils.run_pylint_validation(st.session_state['python_canvases'][i], i, PROMPTS),
        lambda i, k: st.session_state['python_canvases'].__setitem__(i, st.session_state[k].getvalue().decode("utf-8")) if st.session_state.get(k) else None
    )
    
    # --- .env ãƒ­ãƒ¼ãƒ‰ã¨ Client åˆæœŸåŒ– ---
    load_dotenv(dotenv_path=st.session_state.get('selected_env_file', env_files[0]), override=True)
    
    project_id = os.getenv(config.GCP_PROJECT_ID_NAME)
    location = os.getenv(config.GCP_LOCATION_NAME, "global") 
    model_id = st.session_state.get('current_model_id', os.getenv(config.GEMINI_MODEL_ID_NAME, "gemini-3-pro-preview"))
    
    # å®šæ•°å€¤ã®å®šç¾©
    INPUT_LIMIT = 1000000
    OUTPUT_LIMIT = 65536
    max_tokens_val = min(int(os.getenv("MAX_TOKEN", "65536")), OUTPUT_LIMIT)

    try:
        client = genai.Client(vertexai=True, project=project_id, location=location)
    except Exception as e:
        st.error(f"Client init error: {e}")
        st.stop()

    st.caption(f"Backend: {model_id} | Location: {location}")

    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤º
    with st.expander("ğŸ›  ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°", expanded=False):
        for log in reversed(st.session_state["debug_logs"]):
            st.text(log)

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if not st.session_state['system_role_defined']:
        st.subheader("AIã®å½¹å‰²ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚‚ã€å¤‰æ›´ã—ã¦ã‚‚ã©ã¡ã‚‰ã§ã‚‚OKï¼‰")
        role = st.text_area("System Role", value=PROMPTS.get("system", {}).get("text", ""), height=200)
        if st.button("ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹", type="primary"):
            st.session_state['messages'] = [{"role": "system", "content": role}]
            st.session_state['system_role_defined'] = True
            st.rerun()
        st.stop()

    # ä¼šè©±è¡¨ç¤º
    for msg in st.session_state['messages']:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                
                # Groundingã‚½ãƒ¼ã‚¹ã®è¡¨ç¤ºï¼ˆå±¥æ­´ã«ã‚ã‚‹å ´åˆï¼‰
                if "grounding_metadata" in msg and msg["grounding_metadata"]:
                    with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                        st.json(msg["grounding_metadata"])

                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è©³ç´°è¡¨ç¤º
                if msg["role"] == "assistant" and "usage" in msg:
                    u = msg["usage"]
                    in_p = (u['input_tokens'] / INPUT_LIMIT) * 100
                    out_p = (u['output_tokens'] / OUTPUT_LIMIT) * 100
                    
                    st.caption(
                        f"ğŸ“Š **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è©³ç´°**\n\n"
                        f"ğŸ“¥ **Input (Context):** {u['input_tokens']:,} / {INPUT_LIMIT:,} ({in_p:.2f}%)\n"
                        f"ğŸ“¤ **Output (Response):** {u['output_tokens']:,} / {OUTPUT_LIMIT:,} ({out_p:.2f}%)"
                    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆ
    if st.session_state['total_usage']['total_tokens'] > 0:
        st.divider()
        st.caption(f"ğŸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³: {st.session_state['total_usage']['total_tokens']:,}")

    # --- æ©Ÿèƒ½æ”¹å–„: å…¥åŠ›ã‚¨ãƒªã‚¢ã®åˆ†å² (é€šå¸¸ vs ãƒ‰ãƒ©ãƒ•ãƒˆå¾©å…ƒãƒ¢ãƒ¼ãƒ‰) ---
    
    # ä¸­æ–­ã‹ã‚‰ã®å¾©å¸°ãƒ­ã‚¸ãƒƒã‚¯:
    # ç›´å‰ã®ã‚¿ãƒ¼ãƒ³ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã§çµ‚ã‚ã£ã¦ãŠã‚Šã€ã‹ã¤ä»Š generating ã§ãªã‘ã‚Œã°ã€ãã‚Œã¯ã€Œä¸­æ–­ã•ã‚ŒãŸã€ã‚‚ã®ã¨ã¿ãªã™
    if st.session_state.get('messages') and st.session_state['messages'][-1]['role'] == 'user' and not st.session_state.get('is_generating'):
        recover_interrupted_session()
        st.rerun()

    if 'draft_input' in st.session_state:
        # --- ãƒªã‚«ãƒãƒªãƒ¼ãƒ¢ãƒ¼ãƒ‰ (å¾©å…ƒã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç·¨é›†) ---
        st.warning("âš ï¸ å‰å›ã®é€ä¿¡ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚")
        with st.form("draft_form"):
            draft_text = st.text_area("ç·¨é›†ã—ã¦å†é€ä¿¡", value=st.session_state['draft_input'], height=150)
            c1, c2 = st.columns([1, 4])
            with c1:
                resend = st.form_submit_button("å†é€ä¿¡", type="primary", use_container_width=True)
            with c2:
                cancel_draft = st.form_submit_button("ç ´æ£„ (å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢)", use_container_width=True)
            
            if resend:
                st.session_state['messages'].append({"role": "user", "content": draft_text})
                del st.session_state['draft_input']
                st.session_state['is_generating'] = True
                st.rerun()
            elif cancel_draft:
                del st.session_state['draft_input']
                st.rerun()
    
    else:
        # --- é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ ---
        if prompt := st.chat_input("æŒ‡ç¤ºã‚’å…¥åŠ›...", disabled=st.session_state['is_generating']):
            st.session_state['messages'].append({"role": "user", "content": prompt})
            st.session_state['is_generating'] = True
            st.rerun()

    # ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
    if st.session_state['is_generating']:
        # --- æ©Ÿèƒ½æ”¹å–„: åœæ­¢ãƒœã‚¿ãƒ³ã®è¡¨ç¤º ---
        # ç”Ÿæˆä¸­ã¯ãƒãƒ£ãƒƒãƒˆæ¬„ãŒç„¡åŠ¹åŒ–ã•ã‚Œã‚‹ãŸã‚ã€ã“ã“ã«åœæ­¢ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤ºã—ã¾ã™ã€‚
        # æ³¨æ„: Streamlitã®ä»•æ§˜ä¸Šã€ã“ã“ã§ã®ãƒœã‚¿ãƒ³ã‚¯ãƒªãƒƒã‚¯ã¯ã€Œæ¬¡ã®Rerunã€ã‚’å¼•ãèµ·ã“ã—ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä¸­æ–­ã•ã›ã¾ã™ã€‚
        st.markdown("---")
        c_stop, c_info = st.columns([1, 5])
        with c_stop:
            # type="primary" ã§èµ¤ãç›®ç«‹ãŸã›ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ãŒã€é…ç½®ã§ç¤ºã—ã¾ã™
            if st.button("â–  é€ä¿¡å–ã‚Šæ¶ˆã—", key="stop_generating_btn", type="primary"):
                # ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã‚‹ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã“ã“ã§å†å®Ÿè¡Œ(Rerun)ã•ã‚Œã¾ã™ã€‚
                # ç”Ÿæˆå‡¦ç†ã¯ä¸­æ–­ã•ã‚Œã¾ã™ã€‚
                # Rerunå¾Œã€ä¸Šè¨˜ã®ã€Œä¸­æ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒã‚§ãƒƒã‚¯ã€ãŒä½œå‹•ã—ã€ãƒ†ã‚­ã‚¹ãƒˆãŒå¾©å…ƒã•ã‚Œã¾ã™ã€‚
                st.session_state['is_generating'] = False
                st.rerun()
        with c_info:
            st.info("ç”Ÿæˆä¸­... ã€Œé€ä¿¡å–ã‚Šæ¶ˆã—ã€ã‚’æŠ¼ã™ã¨ä¸­æ–­ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾©å…ƒã—ã¾ã™ã€‚")

        with st.chat_message("assistant"):
            # --- æ©Ÿèƒ½æ”¹å–„â‘¢: Thinking & Grounding Process è¡¨ç¤ºã‚¨ãƒªã‚¢ ---
            thought_area_container = st.empty()
            with thought_area_container.container():
                thought_status = st.status("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ (Thinking Process)...", expanded=False)
                thought_placeholder = thought_status.empty()
            # -----------------------------------------------------

            text_placeholder = st.empty()
            full_response = ""
            
            # æ€è€ƒãƒ­ã‚°ï¼ˆThoughtãƒ†ã‚­ã‚¹ãƒˆ + æ¤œç´¢ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼‰ã‚’ã¾ã¨ã‚ã‚‹æ–‡å­—åˆ—
            full_thought_log = ""
            
            usage_metadata = None 
            grounding_chunks = []
            
            # --- ç‰¹æ®Šç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ï¼ˆPylintæ¤œè¨¼ç­‰ï¼‰ã‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‹ã®åˆ¤å®š ---
            is_special_mode = 'special_generation_messages' in st.session_state and st.session_state['special_generation_messages']
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ±ºå®š
            target_messages = []
            if is_special_mode:
                target_messages = st.session_state['special_generation_messages']
                add_debug_log("Generating response for SPECIAL validation request.")
            else:
                target_messages = st.session_state['messages']

            chat_contents = []
            system_instruction = ""
            for m in target_messages:
                if m["role"] == "system":
                    system_instruction = m["content"]
                else:
                    chat_contents.append(types.Content(role=m["role"], parts=[types.Part.from_text(text=m["content"])]))
            
            # --- ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜å‡¦ç† (ä»Šå›ã®ã‚¿ãƒ¼ãƒ³) ---
            file_attachments_meta = []
            
            queue_files = st.session_state.get('uploaded_file_queue', [])
            
            if not is_special_mode and st.session_state.get('uploaded_file_queue'):
                file_parts, file_meta = utils.process_uploaded_files_for_gemini(st.session_state['uploaded_file_queue'])
                
                if file_parts and chat_contents:
                    last_user_msg_content = chat_contents[-1]
                    if last_user_msg_content.role == "user":
                        last_user_msg_content.parts = file_parts + last_user_msg_content.parts
                        file_attachments_meta = file_meta
                        add_debug_log(f"Attached {len(file_parts)} files to the request.")

            # Canvasã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æŒ¿å…¥
            if not is_special_mode:
                context_parts = []
                for i, code in enumerate(st.session_state['python_canvases']):
                    if code.strip() and code != config.ACE_EDITOR_DEFAULT_CODE:
                        context_parts.append(types.Part.from_text(text=f"\n[Canvas-{i+1}]\n```python\n{code}\n```"))
                
                if context_parts and chat_contents:
                    chat_contents[-1].parts = context_parts + chat_contents[-1].parts

            effort = st.session_state.get('reasoning_effort', 'high')
            t_level = types.ThinkingLevel.HIGH if effort == 'high' else types.ThinkingLevel.LOW

            # --- Toolè¨­å®š (Google Search) ---
            tools_config = []
            if st.session_state.get('enable_google_search', False) and not is_special_mode:
                add_debug_log("Google Search Tool Enabled.")
                tools_config = [types.Tool(google_search=types.GoogleSearch())]

            try:
                add_debug_log(f"Requesting stream: {model_id} via {location} (max_output={max_tokens_val})")
                
                gen_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    max_output_tokens=max_tokens_val,
                    tools=tools_config
                )
                if "gemini-3" in model_id:
                    gen_config.thinking_config = types.ThinkingConfig(
                        thinking_level=t_level,
                        include_thoughts=True
                    )

                stream = client.models.generate_content_stream(
                    model=model_id,
                    contents=chat_contents,
                    config=gen_config
                )

                chunk_count = 0
                for chunk in stream:
                    chunk_count += 1
                    if chunk.usage_metadata:
                        usage_metadata = chunk.usage_metadata
                    
                    if not chunk.candidates: continue
                    
                    cand = chunk.candidates[0]

                    # --- Grounding Metadata (æ¤œç´¢ã‚¢ã‚¯ã‚·ãƒ§ãƒ³) ã®å‡¦ç† ---
                    if cand.grounding_metadata:
                        grounding_chunks.append(cand.grounding_metadata)
                        
                        if cand.grounding_metadata.web_search_queries:
                            queries = cand.grounding_metadata.web_search_queries
                            add_debug_log(f"[Grounding] Queries detected: {queries}")
                            for query in queries:
                                action_text = f"\n\nğŸ” **Action (Google Search):** `{query}`\n\n"
                                full_thought_log += action_text
                                thought_placeholder.markdown(full_thought_log)

                    # --- Content Parts ã®å‡¦ç† ---
                    if cand.content and cand.content.parts:
                        for part in cand.content.parts:
                            is_thought = False
                            thought_text = ""

                            if hasattr(part, 'thought') and isinstance(part.thought, str) and part.thought:
                                is_thought = True
                                thought_text = part.thought
                            
                            elif hasattr(part, 'thought') and part.thought is True:
                                is_thought = True
                                thought_text = part.text

                            if is_thought:
                                if thought_text:
                                    full_thought_log += thought_text
                                    thought_placeholder.markdown(full_thought_log)
                            
                            elif part.text:
                                full_response += part.text
                                text_placeholder.markdown(full_response + "â–Œ")
                
                text_placeholder.markdown(full_response)
                
                if not full_thought_log:
                    thought_area_container.empty()
                else:
                    thought_status.update(label="æ€è€ƒå®Œäº† (Finished Thinking)", state="complete", expanded=False)
                
                final_grounding_metadata = None
                if grounding_chunks:
                    last_meta = grounding_chunks[-1]
                    final_grounding_metadata = {}
                    if last_meta.grounding_chunks:
                         sources = []
                         for gc in last_meta.grounding_chunks:
                             if gc.web:
                                 sources.append({"title": gc.web.title, "uri": gc.web.uri})
                         if sources:
                             final_grounding_metadata["sources"] = sources
                            
                    if last_meta.web_search_queries:
                        final_grounding_metadata["queries"] = last_meta.web_search_queries
                    
                    if final_grounding_metadata:
                        with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                            st.json(final_grounding_metadata)

                add_debug_log("Stream successfully finished.")

                current_usage = None
                if usage_metadata:
                    current_usage = {
                        "total_tokens": usage_metadata.total_token_count,
                        "input_tokens": usage_metadata.prompt_token_count,
                        "output_tokens": usage_metadata.candidates_token_count
                    }
                    st.session_state['total_usage']['total_tokens'] += usage_metadata.total_token_count
                    st.session_state['last_usage_info'] = current_usage

                assistant_msg = {"role": "assistant", "content": full_response}
                if current_usage:
                    assistant_msg["usage"] = current_usage
                if final_grounding_metadata:
                    assistant_msg["grounding_metadata"] = final_grounding_metadata
                
                # --- å±¥æ­´ã¸ã®ä¿å­˜å‡¦ç† ---
                if is_special_mode:
                    for m in target_messages:
                        if m["role"] == "user":
                            st.session_state['messages'].append(m)
                    
                    st.session_state['messages'].append(assistant_msg)
                    del st.session_state['special_generation_messages']
                    add_debug_log("Special validation messages merged to history.")
                else:
                    st.session_state['messages'].append(assistant_msg)
                
                # é€ä¿¡å¾…ã¡ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªã‚¢ï¼ˆæ­£å¸¸çµ‚äº†æ™‚ã®ã¿ï¼‰
                if 'uploaded_file_queue' in st.session_state:
                     st.session_state['uploaded_file_queue'] = []

            except Exception as e:
                # ä¸­æ–­(Stop)ã®å ´åˆã‚‚ã“ã“ã«æ¥ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ãŒã€
                # Stopãƒœã‚¿ãƒ³ã«ã‚ˆã‚‹Rerunã®å ´åˆã¯ä¾‹å¤–ã®å‰ã«ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒåœæ­¢ã™ã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚
                st.error(f"Error during generation: {e}")
                add_debug_log(str(e), "error")
            finally:
                st.session_state['is_generating'] = False
                st.rerun()

if __name__ == "__main__":
    run_chatbot_app()
    