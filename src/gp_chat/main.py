import os
import json
import sys
import time
import traceback
import re
import base64

import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from google.genai import errors
from streamlit_ace import st_ace

# --- Local Module Imports ---
try:
    from gp_chat import config
    from gp_chat import utils
    from gp_chat import sidebar
    from gp_chat import data_manager
    from gp_chat import execution_engine
except ImportError:
    import config
    import utils
    import sidebar
    import data_manager
    import execution_engine

# --- Helper Functions ---

def add_debug_log(message, level="info"):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«è¨˜éŒ²ã—ã¾ã™ã€‚"""
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []
    
    timestamp = time.strftime("%H:%M:%S")
    st.session_state["debug_logs"].append(f"[{timestamp}] [{level.upper()}] {message}")
    if len(st.session_state["debug_logs"]) > 50:
        st.session_state["debug_logs"].pop(0)

def load_history(uploader_key):
    """(æ—¢å­˜) Streamlit UploadedFile (JSON) ã‹ã‚‰ä¼šè©±å±¥æ­´ã¨Canvasã‚’å¾©å…ƒã—ã¾ã™ã€‚"""
    uploaded_file = st.session_state.get(uploader_key)
    if not uploaded_file:
        return
    try:
        loaded_data = json.load(uploaded_file)
        if isinstance(loaded_data, dict) and "messages" in loaded_data:
            st.session_state['messages'] = loaded_data["messages"]
            if "python_canvases" in loaded_data:
                st.session_state['python_canvases'] = loaded_data["python_canvases"]
            
            if "multi_code_enabled" in loaded_data:
                st.session_state['multi_code_enabled'] = loaded_data["multi_code_enabled"]

            st.success(config.UITexts.HISTORY_LOADED_SUCCESS)
            st.session_state['system_role_defined'] = True
            st.session_state['canvas_key_counter'] += 1
            
            if 'current_chat_filename' in st.session_state:
                del st.session_state['current_chat_filename']
                
            add_debug_log("Session restored from Uploaded JSON.")

    except Exception as e:
        st.error(f"Load failed: {e}")
        add_debug_log(f"Restore error: {e}", "error")

def load_history_from_local(filename):
    """(æ–°è¦) ãƒ­ãƒ¼ã‚«ãƒ«ã® ./chat_log ãƒ•ã‚©ãƒ«ãƒ€ã«ã‚ã‚‹JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å±¥æ­´ã‚’å¾©å…ƒã—ã¾ã™ã€‚"""
    file_path = os.path.join("chat_log", filename)
    if not os.path.exists(file_path):
        st.error(f"File not found: {file_path}")
        return

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)
        
        if isinstance(loaded_data, dict) and "messages" in loaded_data:
            st.session_state['messages'] = loaded_data["messages"]
            if "python_canvases" in loaded_data:
                st.session_state['python_canvases'] = loaded_data["python_canvases"]
            
            if "multi_code_enabled" in loaded_data:
                st.session_state['multi_code_enabled'] = loaded_data["multi_code_enabled"]

            st.success(f"Loaded: {filename}")
            st.session_state['system_role_defined'] = True
            st.session_state['canvas_key_counter'] += 1
            
            st.session_state['current_chat_filename'] = filename
            
            add_debug_log(f"Session restored from local file: {filename}")
            
    except Exception as e:
        st.error(f"Load failed: {e}")
        add_debug_log(f"Restore error: {e}", "error")

def recover_interrupted_session():
    """
    ä¸­æ–­ã•ã‚ŒãŸã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè¨€ã§çµ‚ã‚ã£ã¦ã„ã‚‹çŠ¶æ…‹ï¼‰ã‚’æ¤œçŸ¥ã—ã€
    å±¥æ­´ã‹ã‚‰å‰Šé™¤ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ‰ãƒ©ãƒ•ãƒˆé ˜åŸŸã«å¾©å…ƒã—ã¾ã™ã€‚
    """
    messages = st.session_state.get('messages', [])
    
    if messages and messages[-1]["role"] == "user":
        last_user_msg = messages.pop()
        content = last_user_msg["content"]
        
        st.session_state['draft_input'] = content
        st.session_state['is_generating'] = False
        
        add_debug_log("Detected interrupted session. Restored draft text.")
        return True
    return False

# --- Streamlit Application ---

def run_chatbot_app():
    st.set_page_config(page_title=config.UITexts.APP_TITLE, layout="wide")
    st.title(config.UITexts.APP_TITLE)
    
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []

    # Initialize Data Manager
    dm = data_manager.SessionDataManager()

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»
    PROMPTS = utils.load_prompts()
    APP_CONFIG = utils.load_app_config()
    supported_extensions = APP_CONFIG.get("file_uploader", {}).get("supported_extensions", [])
    env_files = utils.find_env_files()
    
    if not env_files:
        st.error("env ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        st.stop()

    for key, value in config.SESSION_STATE_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value.copy() if isinstance(value, (dict, list)) else value

    # ä¸­æ–­ãƒªã‚«ãƒãƒªãƒ¼ãƒã‚§ãƒƒã‚¯
    if st.session_state.get('messages') and st.session_state['messages'][-1]['role'] == 'user' and not st.session_state.get('is_generating'):
        recover_interrupted_session()
        st.rerun()

    # --- æ©Ÿèƒ½æ”¹å–„: Canvasèª­ã¿è¾¼ã¿æ™‚ã®æ–‡å­—ã‚³ãƒ¼ãƒ‰å¯¾å¿œé–¢æ•° ---
    def handle_canvas_upload(index, key):
        uploaded_file = st.session_state.get(key)
        if uploaded_file:
            bytes_data = uploaded_file.getvalue()
            text = ""
            try:
                # ã¾ãšUTF-8ã§è©¦ã™
                text = bytes_data.decode("utf-8")
            except UnicodeDecodeError:
                try:
                    # ãƒ€ãƒ¡ãªã‚‰CP932 (Windows Shift-JIS) ã§è©¦ã™
                    text = bytes_data.decode("cp932")
                except UnicodeDecodeError:
                    st.toast("âš ï¸ å¯¾å¿œã—ã¦ã„ãªã„æ–‡å­—ã‚³ãƒ¼ãƒ‰ã§ã™ (UTF-8, CP932ä»¥å¤–)", icon="âŒ")
                    return
            
            st.session_state['python_canvases'][index] = text

    sidebar.render_sidebar(
        supported_extensions, env_files, load_history,
        load_history_from_local,
        lambda i: st.session_state['python_canvases'].__setitem__(i, config.ACE_EDITOR_DEFAULT_CODE),
        lambda i, m: (st.session_state['messages'].append({"role": "user", "content": config.UITexts.REVIEW_PROMPT_MULTI.format(i=i+1) if m else config.UITexts.REVIEW_PROMPT_SINGLE}), st.session_state.__setitem__('is_generating', True)),
        lambda i: utils.run_pylint_validation(st.session_state['python_canvases'][i], i, PROMPTS),
        handle_canvas_upload 
    )
    
    # --- .env ãƒ­ãƒ¼ãƒ‰ã¨ Client åˆæœŸåŒ– ---
    load_dotenv(dotenv_path=st.session_state.get('selected_env_file', env_files[0]), override=True)
    
    project_id = os.getenv(config.GCP_PROJECT_ID_NAME)
    location = os.getenv(config.GCP_LOCATION_NAME, "global") 
    model_id = st.session_state.get('current_model_id', os.getenv(config.GEMINI_MODEL_ID_NAME, "gemini-3-pro-preview"))
    
    INPUT_LIMIT = 1000000
    OUTPUT_LIMIT = 65536
    max_tokens_val = min(int(os.getenv("MAX_TOKEN", "65536")), OUTPUT_LIMIT)

    try:
        client = genai.Client(vertexai=True, project=project_id, location=location)
    except Exception as e:
        st.error(f"Client init error: {e}")
        st.stop()

    st.caption(f"Backend: {model_id} | Location: {location}")

    with st.expander("ğŸ›  ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°", expanded=False):
        for log in reversed(st.session_state["debug_logs"]):
            st.text(log)

    if not st.session_state['system_role_defined']:
        st.subheader("AIã®å½¹å‰²ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚‚ã€å¤‰æ›´ã—ã¦ã‚‚ã©ã¡ã‚‰ã§ã‚‚OKï¼‰")
        role = st.text_area("System Role", value=PROMPTS.get("system", {}).get("text", ""), height=200)
        if st.button("ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹", type="primary"):
            st.session_state['messages'] = [{"role": "system", "content": role}]
            st.session_state['system_role_defined'] = True
            st.rerun()
        st.stop()

    for msg in st.session_state['messages']:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                
                # --- ç”»åƒ (ã‚°ãƒ©ãƒ•) ã®è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ ---
                if "images" in msg and msg["images"]:
                    for img_b64 in msg["images"]:
                        try:
                            st.image(base64.b64decode(img_b64), use_container_width=True)
                        except Exception as e:
                            st.error(f"ç”»åƒè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                # -------------------------------

                if "grounding_metadata" in msg and msg["grounding_metadata"]:
                    with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                        st.json(msg["grounding_metadata"])

                if msg["role"] == "assistant" and "usage" in msg:
                    u = msg["usage"]
                    in_p = (u['input_tokens'] / INPUT_LIMIT) * 100
                    out_p = (u['output_tokens'] / OUTPUT_LIMIT) * 100
                    
                    st.caption(
                        f"ğŸ“Š **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è©³ç´°**\n\n"
                        f"ğŸ“¥ **Input (Context):** {u['input_tokens']:,} / {INPUT_LIMIT:,} ({in_p:.2f}%)\n"
                        f"ğŸ“¤ **Output (Response):** {u['output_tokens']:,} / {OUTPUT_LIMIT:,} ({out_p:.2f}%)"
                    )

    if st.session_state['total_usage']['total_tokens'] > 0:
        st.divider()
        st.caption(f"ğŸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³: {st.session_state['total_usage']['total_tokens']:,}")

    if 'draft_input' in st.session_state:
        st.warning("âš ï¸ å‰å›ã®é€ä¿¡ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾©å…ƒã—ã¾ã—ãŸã€‚")
        with st.form("draft_form"):
            draft_text = st.text_area("ç·¨é›†ã—ã¦å†é€ä¿¡", value=st.session_state['draft_input'], height=150)
            c1, c2 = st.columns([1, 4])
            with c1:
                resend = st.form_submit_button("å†é€ä¿¡", type="primary", use_container_width=True)
            with c2:
                cancel_draft = st.form_submit_button("ç ´æ£„ (å…¥åŠ›ã‚’ã‚¯ãƒªã‚¢)", use_container_width=True)
            
            if resend:
                st.session_state['messages'].append({"role": "user", "content": draft_text})
                del st.session_state['draft_input']
                st.session_state['is_generating'] = True
                st.rerun()
            elif cancel_draft:
                del st.session_state['draft_input']
                st.rerun()
    
    else:
        if prompt := st.chat_input("æŒ‡ç¤ºã‚’å…¥åŠ›...", disabled=st.session_state['is_generating']):
            st.session_state['messages'].append({"role": "user", "content": prompt})
            st.session_state['is_generating'] = True
            st.rerun()

    if st.session_state['is_generating']:
        st.markdown("---")
        c_stop, c_info = st.columns([1, 5])
        with c_stop:
            if st.button("â–  é€ä¿¡å–ã‚Šæ¶ˆã—", key="stop_generating_btn", type="primary"):
                st.session_state['is_generating'] = False
                st.rerun()
        with c_info:
            st.info("ç”Ÿæˆä¸­... ã€Œé€ä¿¡å–ã‚Šæ¶ˆã—ã€ã‚’æŠ¼ã™ã¨ä¸­æ–­ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å¾©å…ƒã—ã¾ã™ã€‚")

        with st.chat_message("assistant"):
            thought_area_container = st.empty()
            with thought_area_container.container():
                thought_status = st.status("æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ (Thinking Process)...", expanded=False)
                thought_placeholder = thought_status.empty()
            
            text_placeholder = st.empty()
            full_response = ""
            full_thought_log = ""
            usage_metadata = None 
            grounding_chunks = []
            
            is_special_mode = 'special_generation_messages' in st.session_state and st.session_state['special_generation_messages']
            
            target_messages = []
            if is_special_mode:
                target_messages = st.session_state['special_generation_messages']
                add_debug_log("Generating response for SPECIAL validation request.")
            else:
                target_messages = st.session_state['messages']

            chat_contents = []
            system_instruction = ""
            for m in target_messages:
                if m["role"] == "system":
                    system_instruction = m["content"]
                else:
                    chat_contents.append(types.Content(role=m["role"], parts=[types.Part.from_text(text=m["content"])]))
            
            file_attachments_meta = []
            queue_files = st.session_state.get('uploaded_file_queue', []) + st.session_state.get('clipboard_queue', [])
            
            # --- Analysis Path: ç‰©ç†ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ (å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ONæ™‚ã®ã¿) ---
            available_files_map = {}
            if st.session_state.get('auto_plot_enabled', False) and not is_special_mode:
                for f in queue_files:
                    try:
                        # æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå‡¦ç†ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã€data_managerå†…ã§seekç®¡ç†æ¸ˆã¿
                        f_path, f_name = dm.save_file(f)
                        if f_path:
                            available_files_map[f_name] = f_path
                            add_debug_log(f"Saved temp file for analysis: {f_name}")
                    except Exception as e:
                        add_debug_log(f"Failed to save temp file {f.name}: {e}", "error")

            # --- Context Path: æ—¢å­˜ã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºå‡¦ç† (å¤‰æ›´ãªã—) ---
            if not is_special_mode and queue_files:
                file_parts, file_meta = utils.process_uploaded_files_for_gemini(queue_files)
                if file_parts and chat_contents:
                    last_user_msg_content = chat_contents[-1]
                    if last_user_msg_content.role == "user":
                        last_user_msg_content.parts = file_parts + last_user_msg_content.parts
                        file_attachments_meta = file_meta
                        add_debug_log(f"Attached {len(file_parts)} files to the request.")

            if not is_special_mode:
                context_parts = []
                for i, code in enumerate(st.session_state['python_canvases']):
                    if code.strip() and code != config.ACE_EDITOR_DEFAULT_CODE:
                        context_parts.append(types.Part.from_text(text=f"\n[Canvas-{i+1}]\n```python\n{code}\n```"))
                
                if context_parts and chat_contents:
                    chat_contents[-1].parts = context_parts + chat_contents[-1].parts

            effort = st.session_state.get('reasoning_effort', 'high')
            t_level = types.ThinkingLevel.HIGH if effort == 'high' else types.ThinkingLevel.LOW

            tools_config = []
            if st.session_state.get('enable_google_search', False) and not is_special_mode:
                add_debug_log("Google Search Tool Enabled.")
                tools_config = [types.Tool(google_search=types.GoogleSearch())]

            try:
                add_debug_log(f"Requesting stream: {model_id} via {location} (max_output={max_tokens_val})")
                
                gen_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    max_output_tokens=max_tokens_val,
                    tools=tools_config
                )
                if "gemini-3" in model_id:
                    gen_config.thinking_config = types.ThinkingConfig(
                        thinking_level=t_level,
                        include_thoughts=True
                    )

                # åˆå›ç”Ÿæˆ
                stream = client.models.generate_content_stream(
                    model=model_id,
                    contents=chat_contents,
                    config=gen_config
                )

                chunk_count = 0
                for chunk in stream:
                    chunk_count += 1
                    if chunk.usage_metadata:
                        usage_metadata = chunk.usage_metadata
                    
                    if not chunk.candidates: continue
                    
                    cand = chunk.candidates[0]

                    if cand.grounding_metadata:
                        grounding_chunks.append(cand.grounding_metadata)
                        if cand.grounding_metadata.web_search_queries:
                            queries = cand.grounding_metadata.web_search_queries
                            add_debug_log(f"[Grounding] Queries detected: {queries}")
                            for query in queries:
                                action_text = f"\n\nğŸ” **Action (Google Search):** `{query}`\n\n"
                                full_thought_log += action_text
                                thought_placeholder.markdown(full_thought_log)

                    if cand.content and cand.content.parts:
                        for part in cand.content.parts:
                            is_thought = False
                            thought_text = ""
                            if hasattr(part, 'thought') and isinstance(part.thought, str) and part.thought:
                                is_thought = True
                                thought_text = part.thought
                            elif hasattr(part, 'thought') and part.thought is True:
                                is_thought = True
                                thought_text = part.text

                            if is_thought:
                                if thought_text:
                                    full_thought_log += thought_text
                                    thought_placeholder.markdown(full_thought_log)
                            elif part.text:
                                full_response += part.text
                                text_placeholder.markdown(full_response + "â–Œ")
                
                text_placeholder.markdown(full_response)
                
                if not full_thought_log:
                    thought_area_container.empty()
                else:
                    thought_status.update(label="æ€è€ƒå®Œäº† (Finished Thinking)", state="complete", expanded=False)
                
                final_grounding_metadata = None
                if grounding_chunks:
                    last_meta = grounding_chunks[-1]
                    final_grounding_metadata = {}
                    if last_meta.grounding_chunks:
                        sources = []
                        for gc in last_meta.grounding_chunks:
                            if gc.web:
                                sources.append({"title": gc.web.title, "uri": gc.web.uri})
                        if sources:
                            final_grounding_metadata["sources"] = sources
                    if last_meta.web_search_queries:
                        final_grounding_metadata["queries"] = last_meta.web_search_queries
                    if final_grounding_metadata:
                        with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                            st.json(final_grounding_metadata)

                add_debug_log("Stream successfully finished.")

                current_usage = None
                if usage_metadata:
                    current_usage = {
                        "total_tokens": usage_metadata.total_token_count,
                        "input_tokens": usage_metadata.prompt_token_count,
                        "output_tokens": usage_metadata.candidates_token_count
                    }
                    st.session_state['total_usage']['total_tokens'] += usage_metadata.total_token_count
                    st.session_state['last_usage_info'] = current_usage

                assistant_msg = {"role": "assistant", "content": full_response}
                if current_usage:
                    assistant_msg["usage"] = current_usage
                if final_grounding_metadata:
                    assistant_msg["grounding_metadata"] = final_grounding_metadata
                
                if is_special_mode:
                    for m in target_messages:
                        if m["role"] == "user":
                            st.session_state['messages'].append(m)
                    st.session_state['messages'].append(assistant_msg)
                    del st.session_state['special_generation_messages']
                    add_debug_log("Special validation messages merged to history.")
                else:
                    st.session_state['messages'].append(assistant_msg)
                    
                    if st.session_state.get('auto_save_enabled', True):
                        current_file = st.session_state.get('current_chat_filename')
                        new_filename = utils.save_auto_history(
                            st.session_state['messages'],
                            st.session_state['python_canvases'],
                            st.session_state['multi_code_enabled'],
                            client,
                            current_filename=current_file
                        )
                        if new_filename:
                            st.session_state['current_chat_filename'] = new_filename

                if 'uploaded_file_queue' in st.session_state:
                     st.session_state['uploaded_file_queue'] = []
                if 'clipboard_queue' in st.session_state:
                     st.session_state['clipboard_queue'] = []

                # --- å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±åˆ (ãƒ¢ãƒ¼ãƒ‰ONã®å ´åˆ) ---
                auto_plot = st.session_state.get('auto_plot_enabled', False)
                add_debug_log(f"[DEBUG] Auto Plot Enabled: {auto_plot}, Special Mode: {is_special_mode}")

                if auto_plot and not is_special_mode:
                    
                    # ãƒªãƒˆãƒ©ã‚¤åˆ¶å¾¡å¤‰æ•°
                    max_retries = 2
                    retry_count = 0
                    
                    # å®Ÿè¡Œå¯¾è±¡ã‚³ãƒ¼ãƒ‰ã®åˆæœŸåŒ–ï¼ˆåˆå›ã¯ç¾åœ¨ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰æŠ½å‡ºï¼‰
                    current_response_text = full_response
                    
                    while retry_count <= max_retries:
                        
                        # ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
                        code_blocks = re.findall(r"```python\n(.*?)\n```", current_response_text, re.DOTALL)
                        add_debug_log(f"[DEBUG] Retry:{retry_count} Found {len(code_blocks)} Python code blocks.") 
                        
                        target_code = None
                        for code in reversed(code_blocks):
                            if any(k in code for k in ["plt.", "fig", "matplotlib", "pd.", "print(", "dataframe"]):
                                target_code = code
                                break
                        
                        if not target_code:
                            add_debug_log("[DEBUG] No suitable target code found (no plt/pd/print keywords).")
                            break # ã‚³ãƒ¼ãƒ‰ãŒãªã‘ã‚Œã°ãƒ«ãƒ¼ãƒ—çµ‚äº†

                        add_debug_log(f"[DEBUG] Retry:{retry_count} Executing code...") 
                        
                        with st.chat_message("assistant"):
                            status_label = "âš™ï¸ ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œä¸­..." if retry_count == 0 else f"âš™ï¸ ã‚³ãƒ¼ãƒ‰ã‚’ä¿®æ­£ã—ã¦å†å®Ÿè¡Œä¸­ (Retry {retry_count})..."
                            with st.status(status_label, expanded=True) as exec_status:
                                
                                stdout_str, figures = execution_engine.execute_user_code(
                                    target_code,
                                    available_files_map, # ä»Šå›ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰åˆ†
                                    st.session_state['python_canvases']
                                )
                                
                                # --- ã‚¨ãƒ©ãƒ¼åˆ¤å®š (TracebackãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹) ---
                                if "Traceback (most recent call last):" in stdout_str:
                                    is_error = True
                                else:
                                    is_error = False
                                
                                # --- æˆåŠŸæ™‚ ã¾ãŸã¯ ãƒªãƒˆãƒ©ã‚¤ä¸Šé™åˆ°é”æ™‚ ---
                                if not is_error or retry_count >= max_retries:
                                    
                                    add_debug_log(f"[DEBUG] Execution finished (Error: {is_error}). Stdout len: {len(stdout_str)}, Figures: {len(figures)}") 

                                    images_b64 = []
                                    for fig_data in figures:
                                        try:
                                            b64_str = base64.b64encode(fig_data.getvalue()).decode('utf-8')
                                            images_b64.append(b64_str)
                                        except Exception as e:
                                            add_debug_log(f"Image encode error: {e}", "error")

                                    # è¡¨ç¤º
                                    if stdout_str:
                                        st.caption("ğŸ“„ æ¨™æº–å‡ºåŠ›:")
                                        st.text(stdout_str)
                                    
                                    if images_b64:
                                        st.caption(f"ğŸ“Š ç”Ÿæˆã•ã‚ŒãŸã‚°ãƒ©ãƒ• ({len(images_b64)}æš):")
                                        for img_b64 in images_b64:
                                            st.image(base64.b64decode(img_b64), use_container_width=True)

                                    # ä¿å­˜
                                    if stdout_str or images_b64:
                                        content_text = f"Running Code...\n\n```text\n{stdout_str}\n```"
                                        if is_error:
                                            content_text = f"âŒ Execution Failed (Retry limit reached):\n\n```text\n{stdout_str}\n```"
                                        
                                        exec_result_msg = {
                                            "role": "assistant",
                                            "content": content_text,
                                            "images": images_b64 
                                        }
                                        st.session_state['messages'].append(exec_result_msg)
                                        
                                        # è‡ªå‹•ä¿å­˜
                                        if st.session_state.get('auto_save_enabled', True):
                                            current_file = st.session_state.get('current_chat_filename')
                                            utils.save_auto_history(
                                                st.session_state['messages'],
                                                st.session_state['python_canvases'],
                                                st.session_state['multi_code_enabled'],
                                                client,
                                                current_filename=current_file
                                            )

                                        if is_error:
                                            exec_status.update(label="ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ (ä¿®æ­£ä¸èƒ½)", state="error")
                                            st.error("AIã«ã‚ˆã‚‹ã‚³ãƒ¼ãƒ‰è‡ªå‹•ä¿®æ­£ãŒå¤±æ•—ã—ã¾ã—ãŸã€‚")
                                        else:
                                            exec_status.update(label="ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œå®Œäº†", state="complete")
                                    else:
                                        exec_status.update(label="ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œå®Œäº† (å‡ºåŠ›ãªã—)", state="complete")
                                        st.warning("ã‚°ãƒ©ãƒ•ã‚‚æ¨™æº–å‡ºåŠ›ã‚‚ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                                    
                                    break # ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹ (æˆåŠŸ or è«¦ã‚)

                                # --- å¤±æ•—æ™‚ (ãƒªãƒˆãƒ©ã‚¤å®Ÿè¡Œ) ---
                                else:
                                    # ã‚¨ãƒ©ãƒ¼ã‚’æ¤œçŸ¥ã—ãŸã®ã§ã€AIã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦å†ç”Ÿæˆã•ã›ã‚‹
                                    retry_count += 1
                                    error_feedback = f"Code Execution Failed with Error:\n{stdout_str}\n\nPlease fix the code and output the corrected Python code block."
                                    
                                    st.warning(f"âš ï¸ ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã‚’æ¤œçŸ¥ã—ã¾ã—ãŸã€‚AIãŒä¿®æ­£ã‚’è©¦ã¿ã¦ã„ã¾ã™... (Attempt {retry_count}/{max_retries})")
                                    add_debug_log(f"[Auto-Fix] Requesting fix for error: {stdout_str[:100]}...")

                                    # å±¥æ­´ã«ã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¿½åŠ ï¼ˆAIã¸ã®å…¥åŠ›ã¨ã—ã¦ï¼‰
                                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¯è¦‹ã›ãªã„å†…éƒ¨çš„ãªè¿½åŠ ã«ã™ã‚‹æ‰‹ã‚‚ã‚ã‚‹ãŒã€ä»Šå›ã¯å±¥æ­´ã«æ®‹ã™
                                    st.session_state['messages'].append({"role": "system", "content": error_feedback})
                                    
                                    # å†ç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                                    # â€»æ–‡è„ˆï¼ˆContextï¼‰ã‚’ç¶­æŒã™ã‚‹ãŸã‚ã€ç¾åœ¨ã® messages ã‚’ãã®ã¾ã¾ä½¿ã†
                                    
                                    fix_chat_contents = []
                                    for m in st.session_state['messages']:
                                        if m["role"] == "system":
                                            continue 
                                        parts = []
                                        if "images" in m: # éå»ã®ç”»åƒã¯ç„¡è¦–ã™ã‚‹ã‹ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡º
                                             pass
                                        
                                        parts.append(types.Part.from_text(text=m["content"]))
                                        fix_chat_contents.append(types.Content(role=m["role"], parts=parts))

                                    # Generate Correction
                                    try:
                                        fix_response = client.models.generate_content(
                                            model=model_id,
                                            contents=fix_chat_contents,
                                            config=gen_config # åŒã˜è¨­å®šã‚’ä½¿ã†
                                        )
                                        
                                        # ä¿®æ­£å¾Œã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
                                        current_response_text = ""
                                        if fix_response.candidates and fix_response.candidates[0].content.parts:
                                            for part in fix_response.candidates[0].content.parts:
                                                if part.text:
                                                    current_response_text += part.text
                                        
                                        # ä¿®æ­£æ¡ˆã‚’å±¥æ­´ã«è¿½åŠ 
                                        st.session_state['messages'].append({"role": "assistant", "content": current_response_text})
                                        
                                        # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¸ï¼ˆã“ã“ã§æŠ½å‡ºãƒ»å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰

                                    except Exception as e:
                                        st.error(f"Auto-fix generation failed: {e}")
                                        break # APIã‚¨ãƒ©ãƒ¼ç­‰ã¯è«¦ã‚ã‚‹

                else:
                    if not auto_plot:
                         add_debug_log("[DEBUG] Execution skipped because Auto Plot is OFF.")

            except Exception as e:
                st.error(f"Error during generation: {e}")
                add_debug_log(str(e), "error")
            finally:
                st.session_state['is_generating'] = False
                st.rerun()

if __name__ == "__main__":
    run_chatbot_app()
    