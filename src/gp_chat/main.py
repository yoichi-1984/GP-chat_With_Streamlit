import os
import json
import sys
import time
import traceback

import streamlit as st
from dotenv import load_dotenv
from google import genai
from google.genai import types
from google.genai import errors
from streamlit_ace import st_ace

# --- Local Module Imports ---
try:
    from gp_chat import config
    from gp_chat import utils
    from gp_chat import sidebar
except ImportError:
    import config
    import utils
    import sidebar

# --- Helper Functions ---

def add_debug_log(message, level="info"):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚¹ãƒ†ãƒ¼ãƒˆã«è¨˜éŒ²ã—ã¾ã™ã€‚"""
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []
    
    timestamp = time.strftime("%H:%M:%S")
    st.session_state["debug_logs"].append(f"[{timestamp}] [{level.upper()}] {message}")
    if len(st.session_state["debug_logs"]) > 50:
        st.session_state["debug_logs"].pop(0)

def load_history(uploader_key):
    """JSONã‹ã‚‰ä¼šè©±å±¥æ­´ã¨Canvasã‚’å¾©å…ƒã—ã¾ã™ã€‚"""
    uploaded_file = st.session_state.get(uploader_key)
    if not uploaded_file:
        return
    try:
        loaded_data = json.load(uploaded_file)
        if isinstance(loaded_data, dict) and "messages" in loaded_data:
            st.session_state['messages'] = loaded_data["messages"]
            if "python_canvases" in loaded_data:
                st.session_state['python_canvases'] = loaded_data["python_canvases"]
            
            if "multi_code_enabled" in loaded_data:
                st.session_state['multi_code_enabled'] = loaded_data["multi_code_enabled"]

            st.success(config.UITexts.HISTORY_LOADED_SUCCESS)
            st.session_state['system_role_defined'] = True
            st.session_state['canvas_key_counter'] += 1
            add_debug_log("Session restored from JSON.")

    except Exception as e:
        st.error(f"Load failed: {e}")
        add_debug_log(f"Restore error: {e}", "error")


# --- Streamlit Application ---

def run_chatbot_app():
    st.set_page_config(page_title=config.UITexts.APP_TITLE, layout="wide")
    st.title(config.UITexts.APP_TITLE)
    
    if "debug_logs" not in st.session_state:
        st.session_state["debug_logs"] = []

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼æç”»
    PROMPTS = utils.load_prompts()
    APP_CONFIG = utils.load_app_config()
    supported_extensions = APP_CONFIG.get("file_uploader", {}).get("supported_extensions", [])
    env_files = utils.find_env_files()
    
    if not env_files:
        st.error("env ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .env ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¿…è¦ã§ã™ã€‚")
        st.stop()

    for key, value in config.SESSION_STATE_DEFAULTS.items():
        if key not in st.session_state:
            st.session_state[key] = value.copy() if isinstance(value, (dict, list)) else value

    sidebar.render_sidebar(
        supported_extensions, env_files, load_history, 
        lambda i: st.session_state['python_canvases'].__setitem__(i, config.ACE_EDITOR_DEFAULT_CODE),
        lambda i, m: (st.session_state['messages'].append({"role": "user", "content": config.UITexts.REVIEW_PROMPT_MULTI.format(i=i+1) if m else config.UITexts.REVIEW_PROMPT_SINGLE}), st.session_state.__setitem__('is_generating', True)),
        lambda i: utils.run_pylint_validation(st.session_state['python_canvases'][i], i, PROMPTS),
        lambda i, k: st.session_state['python_canvases'].__setitem__(i, st.session_state[k].getvalue().decode("utf-8")) if st.session_state.get(k) else None
    )
    
    # --- .env ãƒ­ãƒ¼ãƒ‰ã¨ Client åˆæœŸåŒ– ---
    load_dotenv(dotenv_path=st.session_state.get('selected_env_file', env_files[0]), override=True)
    
    project_id = os.getenv(config.GCP_PROJECT_ID_NAME)
    location = os.getenv(config.GCP_LOCATION_NAME, "global") 
    model_id = st.session_state.get('current_model_id', os.getenv(config.GEMINI_MODEL_ID_NAME, "gemini-3-pro-preview"))
    
    # å®šæ•°å€¤ã®å®šç¾©
    INPUT_LIMIT = 1000000
    OUTPUT_LIMIT = 65536
    max_tokens_val = min(int(os.getenv("MAX_TOKEN", "65536")), OUTPUT_LIMIT)

    try:
        client = genai.Client(vertexai=True, project=project_id, location=location)
    except Exception as e:
        st.error(f"Client init error: {e}")
        st.stop()

    st.caption(f"Backend: {model_id} | Location: {location}")

    # ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°è¡¨ç¤º
    with st.expander("ğŸ›  ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°", expanded=False):
        for log in reversed(st.session_state["debug_logs"]):
            st.text(log)

    # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®š
    if not st.session_state['system_role_defined']:
        st.subheader("AIã®å½¹å‰²ã‚’è¨­å®š")
        role = st.text_area("System Role", value=PROMPTS.get("system", {}).get("text", ""), height=200)
        if st.button("ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹"):
            st.session_state['messages'] = [{"role": "system", "content": role}]
            st.session_state['system_role_defined'] = True
            st.rerun()
        st.stop()

    # ä¼šè©±è¡¨ç¤º
    for msg in st.session_state['messages']:
        if msg["role"] != "system":
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                
                # Groundingã‚½ãƒ¼ã‚¹ã®è¡¨ç¤ºï¼ˆå±¥æ­´ã«ã‚ã‚‹å ´åˆï¼‰
                if "grounding_metadata" in msg and msg["grounding_metadata"]:
                    with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                        st.json(msg["grounding_metadata"])

                # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã®è©³ç´°è¡¨ç¤º
                if msg["role"] == "assistant" and "usage" in msg:
                    u = msg["usage"]
                    in_p = (u['input_tokens'] / INPUT_LIMIT) * 100
                    out_p = (u['output_tokens'] / OUTPUT_LIMIT) * 100
                    
                    st.caption(
                        f"ğŸ“Š **ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡è©³ç´°**\n\n"
                        f"ğŸ“¥ **Input (Context):** {u['input_tokens']:,} / {INPUT_LIMIT:,} ({in_p:.2f}%)\n"
                        f"ğŸ“¤ **Output (Response):** {u['output_tokens']:,} / {OUTPUT_LIMIT:,} ({out_p:.2f}%)"
                    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆ
    if st.session_state['total_usage']['total_tokens'] > 0:
        st.divider()
        st.caption(f"ğŸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ç´¯è¨ˆä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³: {st.session_state['total_usage']['total_tokens']:,}")

    # å…¥åŠ›
    if prompt := st.chat_input("æŒ‡ç¤ºã‚’å…¥åŠ›...", disabled=st.session_state['is_generating']):
        st.session_state['messages'].append({"role": "user", "content": prompt})
        st.session_state['is_generating'] = True
        st.rerun()

    # ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯
    if st.session_state['is_generating']:
        with st.chat_message("assistant"):
            thought_container = st.container()
            text_placeholder = st.empty()
            full_response = ""
            full_thought = ""
            usage_metadata = None 
            grounding_chunks = []
            
            # --- ç‰¹æ®Šç”Ÿæˆãƒ¢ãƒ¼ãƒ‰ï¼ˆPylintæ¤œè¨¼ç­‰ï¼‰ã‹é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ã‹ã®åˆ¤å®š ---
            is_special_mode = 'special_generation_messages' in st.session_state and st.session_state['special_generation_messages']
            
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ä½¿ç”¨ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’æ±ºå®š
            target_messages = []
            if is_special_mode:
                target_messages = st.session_state['special_generation_messages']
                add_debug_log("Generating response for SPECIAL validation request.")
            else:
                target_messages = st.session_state['messages']

            chat_contents = []
            system_instruction = ""
            for m in target_messages:
                if m["role"] == "system":
                    system_instruction = m["content"]
                else:
                    chat_contents.append(types.Content(role=m["role"], parts=[types.Part.from_text(text=m["content"])]))

            # --- ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜å‡¦ç† (ä»Šå›ã®ã‚¿ãƒ¼ãƒ³) ---
            file_attachments_meta = []
            
            # --- DEBUG: é€ä¿¡å‰ã®ã‚­ãƒ¥ãƒ¼ç¢ºèª ---
            queue_files = st.session_state.get('uploaded_file_queue', [])
            add_debug_log(f"[DEBUG] Queue in main loop: {len(queue_files)} files")
            if queue_files:
                with st.expander("ğŸ›  [DEBUG] File Processing Info", expanded=True):
                    st.write(f"Processing {len(queue_files)} files from queue...")
                    for f in queue_files:
                        st.write(f"- {f.name} ({f.type})")
            # -------------------------------

            if not is_special_mode and st.session_state.get('uploaded_file_queue'):
                file_parts, file_meta = utils.process_uploaded_files_for_gemini(st.session_state['uploaded_file_queue'])
                
                # --- DEBUG: å¤‰æ›çµæœã®ç¢ºèª ---
                add_debug_log(f"[DEBUG] Generated {len(file_parts)} API parts")
                # ---------------------------

                if file_parts and chat_contents:
                    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€å¾Œã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆç›´å‰ã®å…¥åŠ›ï¼‰ã«ãƒ‘ãƒ¼ãƒ„ã‚’è¿½åŠ 
                    last_user_msg_content = chat_contents[-1]
                    if last_user_msg_content.role == "user":
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ãƒ¼ãƒ„ã®å‰ã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ãƒ¼ãƒ„ã‚’æŒ¿å…¥
                        last_user_msg_content.parts = file_parts + last_user_msg_content.parts
                        file_attachments_meta = file_meta
                        add_debug_log(f"Attached {len(file_parts)} files to the request.")

            # Canvasã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æŒ¿å…¥
            if not is_special_mode:
                context_parts = []
                for i, code in enumerate(st.session_state['python_canvases']):
                    if code.strip() and code != config.ACE_EDITOR_DEFAULT_CODE:
                        context_parts.append(types.Part.from_text(text=f"\n[Canvas-{i+1}]\n```python\n{code}\n```"))
                
                if context_parts and chat_contents:
                    chat_contents[-1].parts = context_parts + chat_contents[-1].parts

            effort = st.session_state.get('reasoning_effort', 'high')
            t_level = types.ThinkingLevel.HIGH if effort == 'high' else types.ThinkingLevel.LOW

            # --- Toolè¨­å®š (Google Search) ---
            tools_config = []
            if st.session_state.get('enable_google_search', False) and not is_special_mode:
                add_debug_log("Google Search Tool Enabled.")
                tools_config = [types.Tool(google_search=types.GoogleSearch())]

            try:
                add_debug_log(f"Requesting stream: {model_id} via {location} (max_output={max_tokens_val})")
                
                gen_config = types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    max_output_tokens=max_tokens_val,
                    tools=tools_config
                )
                if "gemini-3" in model_id:
                    gen_config.thinking_config = types.ThinkingConfig(thinking_level=t_level)

                stream = client.models.generate_content_stream(
                    model=model_id,
                    contents=chat_contents,
                    config=gen_config
                )

                for chunk in stream:
                    if chunk.usage_metadata:
                        usage_metadata = chunk.usage_metadata
                    
                    if not chunk.candidates: continue
                    
                    # Grounding Metadataã®åé›†
                    if chunk.candidates[0].grounding_metadata:
                        grounding_chunks.append(chunk.candidates[0].grounding_metadata)

                    for part in chunk.candidates[0].content.parts:
                        if part.thought:
                            full_thought += part.text
                            thought_container.status("Thinking...", expanded=True).write(full_thought)
                        elif part.text:
                            full_response += part.text
                            text_placeholder.markdown(full_response + "â–Œ")
                
                text_placeholder.markdown(full_response)
                
                # Groundingæƒ…å ±ã®çµ±åˆã¨è¡¨ç¤º
                final_grounding_metadata = None
                if grounding_chunks:
                    # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚„é›†ç´„ã—ãŸæƒ…å ±ã‚’ä½¿ç”¨ï¼ˆç°¡æ˜“çš„ã«æœ€å¾Œã®ã‚‚ã®ã‚’æ¡ç”¨ï¼‰
                    # å®Ÿéš›ã®Grounding Metadataã¯WebSearchQueryã‚„Sourceãªã©ãŒå«ã¾ã‚Œã‚‹
                    last_meta = grounding_chunks[-1]
                    
                    # ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºå¯èƒ½ãªå½¢å¼ã«å¤‰æ›
                    final_grounding_metadata = {}
                    if last_meta.grounding_chunks:
                         # æ¤œç´¢çµæœï¼ˆSourceï¼‰ã®æŠ½å‡º
                        sources = []
                        for gc in last_meta.grounding_chunks:
                            if gc.web:
                                sources.append({"title": gc.web.title, "uri": gc.web.uri})
                        if sources:
                            final_grounding_metadata["sources"] = sources
                            
                    if last_meta.web_search_queries:
                        final_grounding_metadata["queries"] = last_meta.web_search_queries
                    
                    if final_grounding_metadata:
                        with st.expander("ğŸ” æ¤œç´¢ã‚½ãƒ¼ã‚¹ (Grounding)"):
                            st.json(final_grounding_metadata)

                add_debug_log("Stream successfully finished.")

                current_usage = None
                if usage_metadata:
                    current_usage = {
                        "total_tokens": usage_metadata.total_token_count,
                        "input_tokens": usage_metadata.prompt_token_count,
                        "output_tokens": usage_metadata.candidates_token_count
                    }
                    st.session_state['total_usage']['total_tokens'] += usage_metadata.total_token_count
                    st.session_state['last_usage_info'] = current_usage

                assistant_msg = {"role": "assistant", "content": full_response}
                if current_usage:
                    assistant_msg["usage"] = current_usage
                if final_grounding_metadata:
                    assistant_msg["grounding_metadata"] = final_grounding_metadata
                
                # --- å±¥æ­´ã¸ã®ä¿å­˜å‡¦ç† ---
                if is_special_mode:
                    for m in target_messages:
                        if m["role"] == "user":
                            st.session_state['messages'].append(m)
                    
                    st.session_state['messages'].append(assistant_msg)
                    del st.session_state['special_generation_messages']
                    add_debug_log("Special validation messages merged to history.")
                else:
                    st.session_state['messages'].append(assistant_msg)

            except Exception as e:
                st.error(f"Error during generation: {e}")
                add_debug_log(str(e), "error")
            finally:
                st.session_state['is_generating'] = False
                st.rerun()

if __name__ == "__main__":
    run_chatbot_app()
    