import json
import time
import streamlit as st
from google.genai import types

# --- Local Module Imports ---
try:
    from gp_chat import state_manager
except ImportError:
    import state_manager

def run_deep_reasoning(client, model_id, gen_config, chat_contents, system_instruction, 
                       text_placeholder, thought_status, thought_placeholder):
    """
    æ¨è«–ç‰¹åŒ–ãƒ¢ãƒ¼ãƒ‰ (Deep Reasoning) ç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    ææ¡ˆA (è‡ªå·±æ‰¹åˆ¤) ã¨ ææ¡ˆB (å¤šè§’çš„ä»®èª¬ã®æ¤œè¨¼) ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã€‚
    
    1. Brainstorming: 3ã¤ã®ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç”Ÿæˆ
    2. Exploration & Critique: å„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ·±æ˜ã‚Šã—ã€å¼±ç‚¹ã‚’è‡ªå·±æ‰¹åˆ¤
    3. Integration: å…¨è©•ä¾¡ã‚’è¸ã¾ãˆãŸæœ€çµ‚çµè«–ã®ç”Ÿæˆ
    
    Returns:
        tuple: (full_response, usage_metadata, combined_grounding_metadata)
    """
    state_manager.add_debug_log("[Deep Reasoning] Starting hybrid reasoning agent...")
    
    total_usage = {"input": 0, "output": 0, "total": 0}
    combined_grounding = {"sources": [], "queries": []}
    full_thought_log = "### ğŸ§  Deep Reasoning Process\n\n"
    
    # Reasoningãƒ¢ãƒ¼ãƒ‰ã®ãƒ™ãƒ¼ã‚¹è¨­å®š (Webæ¤œç´¢ãŒONã®å ´åˆã¯ gen_config.tools ã«å¼•ãç¶™ãŒã‚Œã¦ã„ã‚‹)
    base_config = types.GenerateContentConfig(
        max_output_tokens=gen_config.max_output_tokens,
        temperature=gen_config.temperature,
        thinking_config=types.ThinkingConfig(thinking_level=types.ThinkingLevel.HIGH, include_thoughts=True),
        tools=gen_config.tools # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’é©ç”¨
    )

    def extract_grounding_info(candidate):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰Groundingæƒ…å ±ã‚’æŠ½å‡ºã—ã¦combined_groundingã«è¿½åŠ ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        if candidate and candidate.grounding_metadata:
            g_meta = candidate.grounding_metadata
            if g_meta.web_search_queries:
                combined_grounding["queries"].extend(g_meta.web_search_queries)
            if g_meta.grounding_chunks:
                for chunk in g_meta.grounding_chunks:
                    if chunk.web:
                        if not any(s['uri'] == chunk.web.uri for s in combined_grounding["sources"]):
                            combined_grounding["sources"].append({"title": chunk.web.title, "uri": chunk.web.uri})

    # ---------------------------------------------------------
    # Phase 1: Brainstorming (å¤šè§’çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç«‹æ¡ˆ)
    # ---------------------------------------------------------
    thought_status.update(label="ğŸ¤” å¤šè§’çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è€ƒæ¡ˆä¸­ (Brainstorming)...", state="running")
    full_thought_log += "**[Phase 1: Brainstorming]**\nå•é¡Œè§£æ±ºã®ãŸã‚ã®ç•°ãªã‚‹3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆè§£æ³•ã‚„è¦–ç‚¹ï¼‰ã‚’ç«‹æ¡ˆã—ã¦ã„ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    # æ±ç”¨åŒ–: ã‚³ãƒ³ã‚µãƒ«ã‚¿ãƒ³ãƒˆç¸›ã‚Šã‚’å¤–ã—ã€ç´”ç²‹ãªæ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦å¤šè§’çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è¦æ±‚
    brainstorm_prompt = (
        "ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®è«–ç†çš„æ¨è«–èƒ½åŠ›ã‚’æŒã¤AIã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚\n"
        "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç›´è¿‘ã®è¦æ±‚ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆè§£æ³•ã€è¨­è¨ˆã€ã¾ãŸã¯è¦–ç‚¹ï¼‰ã‚’ç«‹æ¡ˆã—ã¦ãã ã•ã„ã€‚\n"
        "ä¾‹ãˆã°ã€ã€Œå‡¦ç†åŠ¹ç‡ãƒ»ç°¡æ½”ã•ã‚’é‡è¦–ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã€Œç¶²ç¾…æ€§ãƒ»å …ç‰¢æ€§ã‚’é‡è¦–ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ã€Œå‰ææ¡ä»¶ãã®ã‚‚ã®ã‚’ç–‘ã†ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€ãªã©ã€å¤šè§’çš„ãªè¦–ç‚¹ã‹ã‚‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    )
    
    brainstorm_contents = chat_contents[-3:] if len(chat_contents) > 3 else chat_contents
    brainstorm_contents = brainstorm_contents + [types.Content(role="user", parts=[types.Part.from_text(text=brainstorm_prompt)])]
    
    brainstorm_config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "approaches": {
                    "type": "ARRAY",
                    "items": {
                        "type": "OBJECT",
                        "properties": {
                            "name": {"type": "STRING", "description": "ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®çŸ­ã„åç§°"},
                            "description": {"type": "STRING", "description": "ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¦‚è¦ã¨ç‹™ã„"}
                        },
                        "required": ["name", "description"]
                    }
                }
            },
            "required": ["approaches"]
        },
        temperature=0.4, # ã‚¢ã‚¤ãƒ‡ã‚¢å‡ºã—ã®ãŸã‚å°‘ã—ã ã‘é«˜ã‚ã«
        thinking_config=types.ThinkingConfig(thinking_level=types.ThinkingLevel.HIGH, include_thoughts=True),
        tools=gen_config.tools # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’é©ç”¨
    )
    
    approaches = []
    try:
        bs_response = client.models.generate_content(
            model=model_id,
            contents=brainstorm_contents,
            config=brainstorm_config
        )
        
        if bs_response.usage_metadata:
            total_usage["input"] += (bs_response.usage_metadata.prompt_token_count or 0)
            total_usage["output"] += (bs_response.usage_metadata.candidates_token_count or 0)
            
        if bs_response.candidates:
            extract_grounding_info(bs_response.candidates[0])
            
        bs_data = json.loads(bs_response.text)
        approaches = bs_data.get("approaches", [])[:3] # æœ€å¤§3ã¤
        
        for i, app in enumerate(approaches):
            full_thought_log += f"* **ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ{i+1} [{app['name']}]:** {app['description']}\n"
            
        thought_placeholder.markdown(full_thought_log)
        state_manager.add_debug_log(f"[Deep Reasoning] Brainstormed approaches: {[a['name'] for a in approaches]}")
        
    except Exception as e:
        state_manager.add_debug_log(f"[Deep Reasoning] Brainstorming failed: {e}", "error")
        approaches = [{"name": "è«–ç†çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ", "description": "ä¸ãˆã‚‰ã‚ŒãŸåˆ¶ç´„ã®ä¸­ã§è«–ç†çš„ã«å•é¡Œã‚’è§£æ±ºã™ã‚‹æ¨™æº–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ"}]
        full_thought_log += f"âš ï¸ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚æ¨™æº–çš„ãªæ¨è«–ã§é€²è¡Œã—ã¾ã™ã€‚\n\n"

    # ---------------------------------------------------------
    # Phase 2: Exploration & Critique (æ·±æ˜ã‚Šã¨è‡ªå·±æ‰¹åˆ¤)
    # ---------------------------------------------------------
    full_thought_log += "\n**[Phase 2: Exploration & Critique]**\nå„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ·±ãæ¤œè¨¼ã—ã€æ½œåœ¨çš„ãªå•é¡Œç‚¹ã‚’è‡ªå·±æ‰¹åˆ¤ï¼ˆCritiqueï¼‰ã—ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    critique_results = []
    
    critique_config = types.GenerateContentConfig(
        temperature=0.2, # è©•ä¾¡ã¯å³å¯†ã«
        thinking_config=types.ThinkingConfig(thinking_level=types.ThinkingLevel.HIGH, include_thoughts=True),
        tools=gen_config.tools # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’é©ç”¨
    )
    
    for i, app in enumerate(approaches):
        thought_status.update(label=f"âš–ï¸ ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¤œè¨¼ãƒ»æ‰¹åˆ¤ {i+1}/{len(approaches)}...", state="running")
        full_thought_log += f"\n* ğŸ” **æ¤œè¨¼ä¸­:** {app['name']}\n"
        thought_placeholder.markdown(full_thought_log)
        
        critique_prompt = (
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å¯¾ã™ã‚‹è§£æ±ºç­–ã¨ã—ã¦ã€ä»¥ä¸‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¤œè¨ã—ã¦ã„ã¾ã™ã€‚\n"
            f"ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒåã€‘: {app['name']}\n"
            f"ã€æ¦‚è¦ã€‘: {app['description']}\n\n"
            "ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ·±ãæ¨è«–ã—ã¦å…·ä½“åŒ–ã—ã€ãã®å¾Œã«**ã‚ãˆã¦å³ã—ãè‡ªå·±æ‰¹åˆ¤ï¼ˆæ½œåœ¨çš„ãªãƒªã‚¹ã‚¯ã€è«–ç†ã®é£›èºã€ã‚¨ãƒƒã‚¸ã‚±ãƒ¼ã‚¹ã§ã®ç ´ç¶»ãªã©ï¼‰**ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n"
            "ã€Œå…·ä½“åŒ–ã•ã‚ŒãŸæ¨è«–ã€ã¨ã€Œè‡ªå·±æ‰¹åˆ¤ãƒ»å¼±ç‚¹ã€ã®2ã¤ã‚’æ˜ç¢ºã«åˆ†ã‘ã¦è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
        )
        
        try:
            cr_response = client.models.generate_content(
                model=model_id,
                contents=chat_contents + [types.Content(role="user", parts=[types.Part.from_text(text=critique_prompt)])],
                config=critique_config
            )
            
            if cr_response.usage_metadata:
                total_usage["input"] += (cr_response.usage_metadata.prompt_token_count or 0)
                total_usage["output"] += (cr_response.usage_metadata.candidates_token_count or 0)

            if cr_response.candidates:
                extract_grounding_info(cr_response.candidates[0])

            result_text = cr_response.text
            critique_results.append(f"ã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ: {app['name']} ã®æ¤œè¨¼ã¨è‡ªå·±æ‰¹åˆ¤ã€‘\n{result_text}")
            
            # é•·ã™ãã‚‹å ´åˆã¯UIè¡¨ç¤ºã‚’åˆ‡ã‚Šè©°ã‚ã‚‹
            disp_text = result_text[:120].replace('\n', ' ') + "..." if len(result_text) > 120 else result_text
            full_thought_log += f"  * ğŸ“ è©•ä¾¡: {disp_text}\n"
            thought_placeholder.markdown(full_thought_log)
            
            time.sleep(1) # APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
            
        except Exception as e:
            state_manager.add_debug_log(f"[Deep Reasoning] Critique failed for '{app['name']}': {e}", "error")
            full_thought_log += f"  * âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\n"
            thought_placeholder.markdown(full_thought_log)

    # ---------------------------------------------------------
    # Phase 3: Integration & Refinement (çµ±åˆã¨æœ€çµ‚å‡ºåŠ›)
    # ---------------------------------------------------------
    thought_status.update(label="ğŸ’¡ å…¨æ¨è«–ã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­ (Integration)...", state="running")
    full_thought_log += "\n**[Phase 3: Integration]**\nå…¨ã¦ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨è‡ªå·±æ‰¹åˆ¤ã‚’è¸ã¾ãˆã€æœ€ã‚‚æ´—ç·´ã•ã‚ŒãŸæœ€çµ‚çµè«–ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    # æ±ç”¨åŒ–: å‡ºåŠ›å½¢å¼ã®ç¸›ã‚Šã‚’ãªãã—ã€ã‚¿ã‚¹ã‚¯ã«æœ€é©åŒ–ã•ã›ã‚‹æŒ‡ç¤ºã«å¤‰æ›´
    compiled_reasoning = "\n\n".join(critique_results)
    synthesis_instruction = system_instruction + (
        "\n\n=================================\n"
        "ã€å³é‡ãªæŒ‡ç¤º: ä»¥ä¸‹ã®ã€Œå¤šè§’çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¤œè¨¼ã¨è‡ªå·±æ‰¹åˆ¤ã®è¨˜éŒ²ã€ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã™ã‚‹æœ€çµ‚çš„ã‹ã¤æœ€ã‚‚æ´—ç·´ã•ã‚ŒãŸå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ã€‘\n"
        "ã€æ¨è«–ã®ãƒ«ãƒ¼ãƒ«ã€‘\n"
        "- å…¨ã¦ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®è‰¯ã„éƒ¨åˆ†ã‚’çµ±åˆã™ã‚‹ã‹ã€ã‚ã‚‹ã„ã¯æœ€ã‚‚æ‰¹åˆ¤ã«è€ãˆã†ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚\n"
        "- æœ€çµ‚çš„ãªå›ç­”ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ï¼ˆã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€ãƒ¬ãƒãƒ¼ãƒˆã€è§£èª¬ã€æ‰‹é †æ›¸ãªã©ï¼‰ã«æœ€ã‚‚é©ã—ãŸå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        "- æœ€çµ‚çµè«–ã«è‡³ã£ãŸè«–ç†çš„æ ¹æ‹ ã‚’ç°¡æ½”ã«å«ã‚ã¤ã¤ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒãã®ã¾ã¾åˆ©ç”¨ã§ãã‚‹å®Ÿç”¨çš„ãªæˆæœç‰©ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚\n\n"
        "ã€æ¤œè¨¼ã¨è‡ªå·±æ‰¹åˆ¤ã®è¨˜éŒ²ã€‘\n"
        f"{compiled_reasoning}\n"
        "=================================\n"
    )
    
    # Synthesisç”¨ã‚³ãƒ³ãƒ•ã‚£ã‚° (ã“ã“ã§æ”¹ã‚ã¦ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã‚’ã‚»ãƒƒãƒˆ)
    synth_config = types.GenerateContentConfig(
        system_instruction=synthesis_instruction,
        max_output_tokens=gen_config.max_output_tokens,
        temperature=0.3,
        thinking_config=types.ThinkingConfig(thinking_level=types.ThinkingLevel.HIGH, include_thoughts=True),
        tools=gen_config.tools # Webæ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’é©ç”¨
    )
    
    full_response = ""
    synth_usage = None
    
    try:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        stream = client.models.generate_content_stream(
            model=model_id,
            contents=chat_contents,
            config=synth_config
        )
        
        for chunk in stream:
            if chunk.usage_metadata:
                synth_usage = chunk.usage_metadata

            if not chunk.candidates: continue
            cand = chunk.candidates[0]
            
            # Synthesisæ™‚ã®Groundingæƒ…å ±ã‚‚è¿½åŠ 
            extract_grounding_info(cand)

            if cand.content and cand.content.parts:
                for part in cand.content.parts:
                    # Thoughtéƒ¨åˆ†ã¯UIã«æµã™
                    is_thought = False
                    thought_text = ""
                    if hasattr(part, 'thought') and isinstance(part.thought, str) and part.thought:
                        is_thought = True
                        thought_text = part.thought
                    elif hasattr(part, 'thought') and part.thought is True:
                        is_thought = True
                        thought_text = part.text

                    if is_thought and thought_text:
                        full_thought_log += thought_text
                        thought_placeholder.markdown(full_thought_log)
                    elif part.text:
                        full_response += part.text
                        text_placeholder.markdown(full_response + "â–Œ")
                        
        text_placeholder.markdown(full_response)
        
        if synth_usage:
            total_usage["input"] += (synth_usage.prompt_token_count or 0)
            total_usage["output"] += (synth_usage.candidates_token_count or 0)
        
    except Exception as e:
        state_manager.add_debug_log(f"[Deep Reasoning] Synthesis failed: {e}", "error")
        st.error(f"Synthesis failed: {e}")
        return "", None, None

    # å®Œäº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    thought_status.update(label="æ¨è«–ç‰¹åŒ–å‡¦ç†å®Œäº† (Deep Reasoning Finished)", state="complete", expanded=False)
    state_manager.add_debug_log("[Deep Reasoning] Agent successfully finished.")

    # è¿”å´ç”¨ã«Usageã‚’æ•´å½¢
    final_usage_metadata = types.GenerateContentResponseUsageMetadata(
        prompt_token_count=total_usage["input"],
        candidates_token_count=total_usage["output"],
        total_token_count=total_usage["input"] + total_usage["output"]
    )

    # Queriesã®é‡è¤‡æ’é™¤
    combined_grounding["queries"] = list(set(combined_grounding["queries"]))

    return full_response, final_usage_metadata, combined_grounding