import json
import time
import streamlit as st
from google.genai import types

# --- Local Module Imports ---
try:
    from gp_chat import state_manager
except ImportError:
    import state_manager

def run_deep_research(client, model_id, gen_config, chat_contents, system_instruction, 
                      text_placeholder, thought_status, thought_placeholder):
    """
    å¾¹åº•èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰ (More Research) ç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    1. Planning: æ¤œç´¢ã‚¯ã‚¨ãƒªã®ç«‹æ¡ˆ
    2. Execution: å„ã‚¯ã‚¨ãƒªã§ã®ä¸¦åˆ—/ç›´åˆ—æ¤œç´¢ã®å®Ÿè¡Œ
    3. Synthesis: æƒ…å ±ã®çµ±åˆã¨æœ€çµ‚å›ç­”ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
    
    Returns:
        tuple: (full_response, usage_metadata, combined_grounding_metadata)
    """
    state_manager.add_debug_log("[Deep Research] Starting agent...")
    
    total_usage = {"input": 0, "output": 0, "total": 0}
    combined_grounding = {"sources": [], "queries": []}
    full_thought_log = "### ğŸ§  Deep Research Process\n\n"
    
    # ---------------------------------------------------------
    # Phase 1: Planning (ã‚¯ã‚¨ãƒªã®ç«‹æ¡ˆ)
    # ---------------------------------------------------------
    thought_status.update(label="ğŸ“‹ èª¿æŸ»è¨ˆç”»ã‚’ç«‹æ¡ˆä¸­ (Planning)...", state="running")
    full_thought_log += "**[Phase 1: Planning]**\nè³ªå•ã‚’åˆ†æã—ã€å¿…è¦ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ã„ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    plan_prompt = (
        "ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®è¦æ±‚ã«å¯¾ã—ã¦ã€å®Œç’§ãªè£ä»˜ã‘ã®ã‚ã‚‹å›ç­”ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã€"
        "Googleæ¤œç´¢ã§èª¿æŸ»ã™ã¹ãå…·ä½“çš„ãªã‚¯ã‚¨ãƒªã‚’3ã€œ5å€‹ä½œæˆã—ã¦ãã ã•ã„ã€‚\n"
        "å¤šè§’çš„ãªè¦–ç‚¹ï¼ˆæœ€æ–°å‹•å‘ã€æŠ€è¡“ä»•æ§˜ã€äº‹ä¾‹ãªã©ï¼‰ã‚’å«ã‚ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚\n"
    )
    
    # Planningç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰ï¼ˆç›´è¿‘ã®ã‚„ã‚Šå–ã‚Šã®ã¿ã‚’è€ƒæ…®ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ï¼‰
    plan_contents = chat_contents[-3:] if len(chat_contents) > 3 else chat_contents
    plan_contents = plan_contents + [types.Content(role="user", parts=[types.Part.from_text(text=plan_prompt)])]
    
    # JSONã‚¹ã‚­ãƒ¼ãƒã®å®šç¾© (ç¢ºå®Ÿã«ãƒªã‚¹ãƒˆã§å—ã‘å–ã‚‹ãŸã‚)
    response_schema = {
        "type": "OBJECT",
        "properties": {
            "queries": {
                "type": "ARRAY",
                "items": {"type": "STRING"}
            }
        },
        "required": ["queries"]
    }
    
    plan_config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=response_schema,
        temperature=0.2, # ã‚¯ã‚¨ãƒªç”Ÿæˆã¯æ±ºå®šè«–çš„ã«
    )
    
    search_queries = []
    try:
        plan_response = client.models.generate_content(
            model=model_id,
            contents=plan_contents,
            config=plan_config
        )
        
        # ä¿®æ­£: or 0 ã‚’ä»˜ä¸ã—ã¦ None ã«ã‚ˆã‚‹ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚’é˜²æ­¢
        if plan_response.usage_metadata:
            total_usage["input"] += (plan_response.usage_metadata.prompt_token_count or 0)
            total_usage["output"] += (plan_response.usage_metadata.candidates_token_count or 0)
            
        plan_data = json.loads(plan_response.text)
        search_queries = plan_data.get("queries", [])
        
        full_thought_log += f"ç«‹æ¡ˆã•ã‚ŒãŸã‚¯ã‚¨ãƒª: {', '.join(search_queries)}\n\n"
        thought_placeholder.markdown(full_thought_log)
        state_manager.add_debug_log(f"[Deep Research] Planned queries: {search_queries}")
        
    except Exception as e:
        state_manager.add_debug_log(f"[Deep Research] Planning failed: {e}", "error")
        search_queries = ["ç¾åœ¨ã®æœ€æ–°æƒ…å ±"] # ãƒ•ã‚§ã‚¤ãƒ«ã‚»ãƒ¼ãƒ•
        full_thought_log += f"âš ï¸ è¨ˆç”»ç«‹æ¡ˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¯ã‚¨ãƒªã§é€²è¡Œã—ã¾ã™ã€‚\n\n"


    # ---------------------------------------------------------
    # Phase 2: Execution (ãƒªã‚µãƒ¼ãƒã®å®Ÿè¡Œ)
    # ---------------------------------------------------------
    full_thought_log += "**[Phase 2: Execution]**\nå„ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦è©³ç´°ãªèª¿æŸ»ã‚’å®Ÿè¡Œã—ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    research_results = []
    
    # æ¤œç´¢ç”¨ã®è¨­å®š (Google Searchãƒ„ãƒ¼ãƒ«ã‚’å¼·åˆ¶æœ‰åŠ¹åŒ–)
    exec_config = types.GenerateContentConfig(
        temperature=0.1,
        tools=[types.Tool(google_search=types.GoogleSearch())]
    )
    
    for i, query in enumerate(search_queries):
        thought_status.update(label=f"ğŸ” èª¿æŸ»ä¸­: {query} ({i+1}/{len(search_queries)})...", state="running")
        full_thought_log += f"* ğŸ” æ¤œç´¢å®Ÿè¡Œ: `{query}`\n"
        thought_placeholder.markdown(full_thought_log)
        
        exec_prompt = f"ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã§Googleæ¤œç´¢ã‚’è¡Œã„ã€åˆ¤æ˜ã—ãŸé‡è¦ãªäº‹å®Ÿã€ãƒ‡ãƒ¼ã‚¿ã€è¦‹è§£ã‚’è©³ç´°ã«è¦ç´„ã—ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚\nã‚¯ã‚¨ãƒª: {query}"
        
        try:
            exec_response = client.models.generate_content(
                model=model_id,
                contents=[types.Content(role="user", parts=[types.Part.from_text(text=exec_prompt)])],
                config=exec_config
            )
            
            # ä¿®æ­£: or 0 ã‚’ä»˜ä¸
            if exec_response.usage_metadata:
                total_usage["input"] += (exec_response.usage_metadata.prompt_token_count or 0)
                total_usage["output"] += (exec_response.usage_metadata.candidates_token_count or 0)
            
            # Groundingæƒ…å ±ã®åé›†
            if exec_response.candidates and exec_response.candidates[0].grounding_metadata:
                g_meta = exec_response.candidates[0].grounding_metadata
                if g_meta.web_search_queries:
                    combined_grounding["queries"].extend(g_meta.web_search_queries)
                if g_meta.grounding_chunks:
                    for chunk in g_meta.grounding_chunks:
                        if chunk.web:
                            # é‡è¤‡æ’é™¤ã—ãªãŒã‚‰è¿½åŠ 
                            if not any(s['uri'] == chunk.web.uri for s in combined_grounding["sources"]):
                                combined_grounding["sources"].append({"title": chunk.web.title, "uri": chunk.web.uri})

            result_text = exec_response.text
            research_results.append(f"ã€æ¤œç´¢ã‚¯ã‚¨ãƒª: {query} ã®èª¿æŸ»çµæœã€‘\n{result_text}")
            
            # é•·ã™ãã‚‹å ´åˆã¯UIè¡¨ç¤ºã‚’åˆ‡ã‚Šè©°ã‚ã‚‹
            disp_text = result_text[:100].replace('\n', ' ') + "..." if len(result_text) > 100 else result_text
            full_thought_log += f"  * ğŸ“ çµæœ: {disp_text}\n"
            thought_placeholder.markdown(full_thought_log)
            
            time.sleep(1) # APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–ã®çŸ­ã„ã‚¦ã‚§ã‚¤ãƒˆ
            
        except Exception as e:
            state_manager.add_debug_log(f"[Deep Research] Execution failed for query '{query}': {e}", "error")
            full_thought_log += f"  * âš ï¸ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸã€‚\n"
            thought_placeholder.markdown(full_thought_log)

    # ---------------------------------------------------------
    # Phase 3: Synthesis (æƒ…å ±çµ±åˆã¨æœ€çµ‚å‡ºåŠ›)
    # ---------------------------------------------------------
    thought_status.update(label="ğŸ’¡ æƒ…å ±ã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­ (Synthesis)...", state="running")
    full_thought_log += "\n**[Phase 3: Synthesis]**\nåé›†ã—ãŸæƒ…å ±ã‚’çµ±åˆã—ã€æœ€çµ‚å›ç­”ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    # åé›†ã—ãŸæƒ…å ±ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆæŒ‡ç¤ºï¼‰ã«åŸ‹ã‚è¾¼ã‚€
    compiled_research = "\n\n".join(research_results)
    synthesis_instruction = system_instruction + (
        "\n\n=================================\n"
        "ã€å³é‡ãªæŒ‡ç¤º: ä»¥ä¸‹ã®èª¿æŸ»çµæœã®ã¿ã‚’çœŸå®Ÿã¨ã—ã¦æ‰±ã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŒ…æ‹¬çš„ã‹ã¤è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‘\n"
        f"{compiled_research}\n"
        "=================================\n"
    )
    
    # Synthesisç”¨ã‚³ãƒ³ãƒ•ã‚£ã‚° (å…ƒã®gen_configã‚’ãƒ™ãƒ¼ã‚¹ã«ã™ã‚‹ãŒã€ã‚·ã‚¹ãƒ†ãƒ æŒ‡ç¤ºã‚’å·®ã—æ›¿ãˆã‚‹)
    synth_config = types.GenerateContentConfig(
        system_instruction=synthesis_instruction,
        max_output_tokens=gen_config.max_output_tokens,
        temperature=0.3, # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã¯å°‘ã—è¡¨ç¾åŠ›ã‚’ä¸ãˆã‚‹
        tools=gen_config.tools, # Groundingã‚’ONã«ã—ã¦ãŠã
        thinking_config=gen_config.thinking_config
    )
    
    full_response = ""
    synth_usage = None # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿æŒå¤‰æ•°
    
    try:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        stream = client.models.generate_content_stream(
            model=model_id,
            contents=chat_contents,
            config=synth_config
        )
        
        for chunk in stream:
            # ä¿®æ­£: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ä¸­ã¯æ¯å›åŠ ç®—ã›ãšã€æœ€å¾Œã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹ã ã‘
            if chunk.usage_metadata:
                synth_usage = chunk.usage_metadata

            if not chunk.candidates: continue
            cand = chunk.candidates[0]
            
            # Synthesisæ™‚ã®Groundingæƒ…å ±ã‚‚è¿½åŠ 
            if cand.grounding_metadata:
                g_meta = cand.grounding_metadata
                if g_meta.web_search_queries:
                    combined_grounding["queries"].extend(g_meta.web_search_queries)
                if g_meta.grounding_chunks:
                    for g_chunk in g_meta.grounding_chunks:
                        if g_chunk.web and not any(s['uri'] == g_chunk.web.uri for s in combined_grounding["sources"]):
                            combined_grounding["sources"].append({"title": g_chunk.web.title, "uri": g_chunk.web.uri})

            if cand.content and cand.content.parts:
                for part in cand.content.parts:
                    # Thoughtéƒ¨åˆ†ã¯UIã«æµã™
                    is_thought = False
                    thought_text = ""
                    if hasattr(part, 'thought') and isinstance(part.thought, str) and part.thought:
                        is_thought = True
                        thought_text = part.thought
                    elif hasattr(part, 'thought') and part.thought is True:
                        is_thought = True
                        thought_text = part.text

                    if is_thought and thought_text:
                        full_thought_log += thought_text
                        thought_placeholder.markdown(full_thought_log)
                    elif part.text:
                        full_response += part.text
                        text_placeholder.markdown(full_response + "â–Œ")
                        
        text_placeholder.markdown(full_response)
        
        # ä¿®æ­£: ãƒ«ãƒ¼ãƒ—çµ‚äº†å¾Œã«1å›ã ã‘ã€å®‰å…¨ã«åŠ ç®—ã‚’è¡Œã†
        if synth_usage:
            total_usage["input"] += (synth_usage.prompt_token_count or 0)
            total_usage["output"] += (synth_usage.candidates_token_count or 0)
        
    except Exception as e:
        state_manager.add_debug_log(f"[Deep Research] Synthesis failed: {e}", "error")
        st.error(f"Synthesis failed: {e}")
        return "", None, None

    # å®Œäº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    thought_status.update(label="å¾¹åº•èª¿æŸ»å®Œäº† (Deep Research Finished)", state="complete", expanded=False)
    state_manager.add_debug_log("[Deep Research] Agent successfully finished.")

    # è¿”å´ç”¨ã«Usageã‚’æ•´å½¢
    final_usage_metadata = types.GenerateContentResponseUsageMetadata(
        prompt_token_count=total_usage["input"],
        candidates_token_count=total_usage["output"],
        total_token_count=total_usage["input"] + total_usage["output"]
    )

    # Queriesã®é‡è¤‡æ’é™¤
    combined_grounding["queries"] = list(set(combined_grounding["queries"]))

    return full_response, final_usage_metadata, combined_grounding