import json
import time
import streamlit as st
from google.genai import types

# --- Local Module Imports ---
try:
    from gp_chat import state_manager
except ImportError:
    import state_manager

def run_deep_research(client, model_id, gen_config, chat_contents, system_instruction, 
                      text_placeholder, thought_status, thought_placeholder):
    """
    å¾¹åº•èª¿æŸ»ãƒ¢ãƒ¼ãƒ‰ (More Research) ç”¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    1. Dynamic Research: æƒ…å ±ã®éä¸è¶³ã‚’è©•ä¾¡ã—ãªãŒã‚‰ã€å‹•çš„ãªæ¤œç´¢ãƒ«ãƒ¼ãƒ—(ReAct)ã‚’å®Ÿè¡Œ
    2. Synthesis: åé›†ã—ãŸå…¨æƒ…å ±ã¨æ¨è«–ãƒ«ãƒ¼ãƒ«ã‚’ç”¨ã„ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆ
    
    Returns:
        tuple: (full_response, usage_metadata, combined_grounding_metadata)
    """
    state_manager.add_debug_log("[Deep Research] Starting dynamic ReAct agent...")
    
    total_usage = {"input": 0, "output": 0, "total": 0}
    combined_grounding = {"sources": [], "queries": []}
    full_thought_log = "### ğŸ§  Deep Research Process\n\n"
    
    # ---------------------------------------------------------
    # Phase 1: Dynamic Research Loop (ReActå‹æ·±æ˜ã‚Š)
    # ---------------------------------------------------------
    full_thought_log += "**[Phase 1: Dynamic Research]**\næƒ…å ±ã®éä¸è¶³ã‚’è©•ä¾¡ã—ãªãŒã‚‰ã€å‹•çš„ã«æ¤œç´¢ã¨æ·±æ˜ã‚Šã‚’ç¹°ã‚Šè¿”ã—ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    MAX_ITERATIONS = 3
    iteration = 0
    research_results = []
    executed_queries = set()
    
    # è©•ä¾¡ãƒ»è¨ˆç”»ç”¨ã®JSONã‚¹ã‚­ãƒ¼ãƒ
    react_config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema={
            "type": "OBJECT",
            "properties": {
                "status": {"type": "STRING", "description": "'needs_more_info' or 'sufficient'"},
                "next_queries": {"type": "ARRAY", "items": {"type": "STRING"}},
                "reasoning": {"type": "STRING", "description": "ç¾åœ¨ã®çŠ¶æ³ã¨æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³(æ¤œç´¢)ã‚’æ±ºå®šã—ãŸç†ç”±"}
            },
            "required": ["status", "next_queries", "reasoning"]
        },
        temperature=0.2,
    )
    
    # æ¤œç´¢å®Ÿè¡Œç”¨ã®ã‚³ãƒ³ãƒ•ã‚£ã‚°
    exec_config = types.GenerateContentConfig(
        temperature=0.1,
        tools=[types.Tool(google_search=types.GoogleSearch())]
    )

    while iteration < MAX_ITERATIONS:
        iteration += 1
        thought_status.update(label=f"ğŸ”„ èª¿æŸ»ã‚µã‚¤ã‚¯ãƒ« {iteration}/{MAX_ITERATIONS} ã‚’å®Ÿè¡Œä¸­...", state="running")
        
        # --- è©•ä¾¡ãƒ»è¨ˆç”» ---
        current_knowledge = "\n\n".join(research_results) if research_results else "ï¼ˆã¾ã èª¿æŸ»çµæœã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"
        
        react_prompt = (
            "ã‚ãªãŸã¯å„ªç§€ãªãƒªã‚µãƒ¼ãƒãƒ£ãƒ¼ã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æœ€æ–°ã®è¦æ±‚ã«å¯¾ã—ã¦ã€å®Œç’§ãªè£ä»˜ã‘ã®ã‚ã‚‹å›ç­”ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æƒ…å ±ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚\n"
            "ã“ã‚Œã¾ã§ã«ä»¥ä¸‹ã®èª¿æŸ»çµæœãŒå¾—ã‚‰ã‚Œã¦ã„ã¾ã™ï¼š\n"
            "-----------------\n"
            f"{current_knowledge}\n"
            "-----------------\n"
            "ã€ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã€‘\n"
            "ä¸Šè¨˜ã®æƒ…å ±ã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«å®Œå…¨ã«ç­”ãˆã‚‹ãŸã‚ã«æƒ…å ±ãŒååˆ†ã‹åˆ¤å®šã—ã¦ãã ã•ã„ã€‚\n"
            "ã‚‚ã—æƒ…å ±ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€äº‹å®Ÿã®è£ä»˜ã‘ãŒå¼±ã„ã€ã¾ãŸã¯æ–°ãŸã«æ·±æ˜ã‚Šã™ã¹ãç–‘å•ç‚¹ãŒæµ®ä¸Šã—ãŸå ´åˆã¯ã€"
            "ãã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®Googleæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’1ã€œ3å€‹ææ¡ˆã—ã¦ãã ã•ã„ã€‚\n"
            "æƒ…å ±ãŒååˆ†ã«æƒã£ãŸã¨åˆ¤æ–­ã—ãŸå ´åˆã¯ã€statusã‚’'sufficient'ã«ã—ã€next_queriesã¯ç©ºã«ã—ã¦ãã ã•ã„ã€‚\n"
        )
        
        # ç›´è¿‘ã®ã‚„ã‚Šå–ã‚Šã‚’å…ƒã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
        react_contents = chat_contents[-3:] if len(chat_contents) > 3 else chat_contents
        react_contents = react_contents + [types.Content(role="user", parts=[types.Part.from_text(text=react_prompt)])]
        
        try:
            react_response = client.models.generate_content(
                model=model_id,
                contents=react_contents,
                config=react_config
            )
            if react_response.usage_metadata:
                total_usage["input"] += (react_response.usage_metadata.prompt_token_count or 0)
                total_usage["output"] += (react_response.usage_metadata.candidates_token_count or 0)
                
            react_data = json.loads(react_response.text)
            status = react_data.get("status", "needs_more_info")
            next_queries = react_data.get("next_queries", [])
            reasoning = react_data.get("reasoning", "")
            
            # AIã®åˆ¤æ–­ç†ç”±ã‚’UIã«è¡¨ç¤º
            full_thought_log += f"\n**[Cycle {iteration}] AIã®æ€è€ƒ:** {reasoning}\n"
            thought_placeholder.markdown(full_thought_log)
            state_manager.add_debug_log(f"[Deep Research] Cycle {iteration} reasoning: {reasoning}")
            
            # çµ‚äº†åˆ¤å®š
            if status == "sufficient" or not next_queries:
                full_thought_log += "âœ… æƒ…å ±ãŒååˆ†ã«æƒã£ãŸã¨åˆ¤æ–­ã—ã¾ã—ãŸã€‚èª¿æŸ»ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚\n"
                thought_placeholder.markdown(full_thought_log)
                break
                
            # --- æ¤œç´¢ã®å®Ÿè¡Œ ---
            # éå»ã«å®Ÿè¡Œã—ãŸã‚¯ã‚¨ãƒªã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€æœ€å¤§3å€‹ã¾ã§ã«åˆ¶é™
            queries_to_run = [q for q in next_queries if q not in executed_queries][:3]
            if not queries_to_run:
                full_thought_log += "âš ï¸ æ–°ã—ã„æ¤œç´¢ã‚¯ã‚¨ãƒªãŒã‚ã‚Šã¾ã›ã‚“ã€‚èª¿æŸ»ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚\n"
                thought_placeholder.markdown(full_thought_log)
                break
                
            for query in queries_to_run:
                executed_queries.add(query)
                full_thought_log += f"* ğŸ” æ¤œç´¢å®Ÿè¡Œ: `{query}`\n"
                thought_placeholder.markdown(full_thought_log)
                
                exec_prompt = f"ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã§Googleæ¤œç´¢ã‚’è¡Œã„ã€åˆ¤æ˜ã—ãŸé‡è¦ãªäº‹å®Ÿã€ãƒ‡ãƒ¼ã‚¿ã€è¦‹è§£ã‚’è©³ç´°ã«è¦ç´„ã—ã¦ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„ã€‚\nã‚¯ã‚¨ãƒª: {query}"
                exec_response = client.models.generate_content(
                    model=model_id,
                    contents=[types.Content(role="user", parts=[types.Part.from_text(text=exec_prompt)])],
                    config=exec_config
                )
                
                if exec_response.usage_metadata:
                    total_usage["input"] += (exec_response.usage_metadata.prompt_token_count or 0)
                    total_usage["output"] += (exec_response.usage_metadata.candidates_token_count or 0)
                
                # Groundingæƒ…å ±ã®åé›†
                if exec_response.candidates and exec_response.candidates[0].grounding_metadata:
                    g_meta = exec_response.candidates[0].grounding_metadata
                    if g_meta.web_search_queries:
                        combined_grounding["queries"].extend(g_meta.web_search_queries)
                    if g_meta.grounding_chunks:
                        for chunk in g_meta.grounding_chunks:
                            if chunk.web:
                                if not any(s['uri'] == chunk.web.uri for s in combined_grounding["sources"]):
                                    combined_grounding["sources"].append({"title": chunk.web.title, "uri": chunk.web.uri})

                result_text = exec_response.text
                research_results.append(f"ã€æ¤œç´¢ã‚¯ã‚¨ãƒª: {query} ã®èª¿æŸ»çµæœã€‘\n{result_text}")
                
                # é•·ã™ãã‚‹å ´åˆã¯UIè¡¨ç¤ºã‚’åˆ‡ã‚Šè©°ã‚ã‚‹
                disp_text = result_text[:100].replace('\n', ' ') + "..." if len(result_text) > 100 else result_text
                full_thought_log += f"  * ğŸ“ çµæœ: {disp_text}\n"
                thought_placeholder.markdown(full_thought_log)
                
                time.sleep(1) # APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
                
        except Exception as e:
            state_manager.add_debug_log(f"[Deep Research] Loop {iteration} failed: {e}", "error")
            full_thought_log += f"âš ï¸ èª¿æŸ»ã‚µã‚¤ã‚¯ãƒ«ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\n"
            thought_placeholder.markdown(full_thought_log)
            break

    # ---------------------------------------------------------
    # Phase 2: Synthesis (æƒ…å ±çµ±åˆã¨æ¨è«–ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ãŸæœ€çµ‚å‡ºåŠ›)
    # ---------------------------------------------------------
    thought_status.update(label="ğŸ’¡ æƒ…å ±ã‚’çµ±åˆã—ã¦æœ€çµ‚å›ç­”ã‚’ç”Ÿæˆä¸­ (Synthesis)...", state="running")
    full_thought_log += "\n**[Phase 2: Synthesis]**\nåé›†ã—ãŸæƒ…å ±ã‚’å³æ ¼ãªæ¨è«–ãƒ«ãƒ¼ãƒ«ã«åŸºã¥ã„ã¦çµ±åˆã—ã€å›ç­”ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™...\n"
    thought_placeholder.markdown(full_thought_log)
    
    # åé›†ã—ãŸæƒ…å ±ã¨ã€Œæ¨è«–ã®èª˜å°ãƒ«ãƒ¼ãƒ«ã€ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€
    compiled_research = "\n\n".join(research_results) if research_results else "ï¼ˆè¿½åŠ ã®èª¿æŸ»çµæœã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"
    synthesis_instruction = system_instruction + (
        "\n\n=================================\n"
        "ã€å³é‡ãªæŒ‡ç¤º: ä»¥ä¸‹ã®èª¿æŸ»çµæœã®ã¿ã‚’çœŸå®Ÿã¨ã—ã¦æ‰±ã„ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«åŒ…æ‹¬çš„ã‹ã¤è«–ç†çš„ã«å›ç­”ã—ã¦ãã ã•ã„ã€‘\n"
        "ã€æ¨è«–ã®ãƒ«ãƒ¼ãƒ«ã€‘\n"
        "- å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚„å…¬çš„æ©Ÿé–¢ã€ä¿¡é ¼æ€§ã®é«˜ã„ä¸€æ¬¡æƒ…å ±ã‚’æœ€å„ªå…ˆã—ã¦è©•ä¾¡ã™ã‚‹ã“ã¨ã€‚\n"
        "- æƒ…å ±æºé–“ã§çŸ›ç›¾ãŒã‚ã‚‹å ´åˆã¯ã€ã©ã¡ã‚‰ã‹ä¸€æ–¹ã‚’ç„¡ç†ã«æ­£è§£ã¨ã™ã‚‹ã®ã§ã¯ãªãã€ä¸¡è«–ã‚’ä½µè¨˜ã—ãŸä¸Šã§ã€èƒŒæ™¯ã‚„å‰ææ¡ä»¶ã‚’æ¨æ¸¬ã—ã¦è«–ç†çš„ã«æ¯”è¼ƒã™ã‚‹ã“ã¨ã€‚\n"
        "- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æ±‚ã«ç„¡é–¢ä¿‚ãªãƒã‚¤ã‚ºæƒ…å ±ã¯ç„¡è¦–ã—ã€çµè«–ã«è‡³ã‚‹è«–ç†å±•é–‹ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã€‚\n\n"
        "ã€èª¿æŸ»çµæœãƒ‡ãƒ¼ã‚¿ã€‘\n"
        f"{compiled_research}\n"
        "=================================\n"
    )
    
    # Synthesisç”¨ã‚³ãƒ³ãƒ•ã‚£ã‚°
    synth_config = types.GenerateContentConfig(
        system_instruction=synthesis_instruction,
        max_output_tokens=gen_config.max_output_tokens,
        temperature=0.3, # çµ±åˆãƒ•ã‚§ãƒ¼ã‚ºã¯å°‘ã—è¡¨ç¾åŠ›ã‚’ä¸ãˆã‚‹
        tools=gen_config.tools, # Groundingã‚’ONã«ç¶­æŒ
        thinking_config=gen_config.thinking_config
    )
    
    full_response = ""
    synth_usage = None
    
    try:
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”Ÿæˆ
        stream = client.models.generate_content_stream(
            model=model_id,
            contents=chat_contents,
            config=synth_config
        )
        
        for chunk in stream:
            if chunk.usage_metadata:
                synth_usage = chunk.usage_metadata

            if not chunk.candidates: continue
            cand = chunk.candidates[0]
            
            # Synthesisæ™‚ã®Groundingæƒ…å ±ã‚‚è¿½åŠ 
            if cand.grounding_metadata:
                g_meta = cand.grounding_metadata
                if g_meta.web_search_queries:
                    combined_grounding["queries"].extend(g_meta.web_search_queries)
                if g_meta.grounding_chunks:
                    for g_chunk in g_meta.grounding_chunks:
                        if g_chunk.web and not any(s['uri'] == g_chunk.web.uri for s in combined_grounding["sources"]):
                            combined_grounding["sources"].append({"title": g_chunk.web.title, "uri": g_chunk.web.uri})

            if cand.content and cand.content.parts:
                for part in cand.content.parts:
                    # Thoughtéƒ¨åˆ†ã¯UIã«æµã™
                    is_thought = False
                    thought_text = ""
                    if hasattr(part, 'thought') and isinstance(part.thought, str) and part.thought:
                        is_thought = True
                        thought_text = part.thought
                    elif hasattr(part, 'thought') and part.thought is True:
                        is_thought = True
                        thought_text = part.text

                    if is_thought and thought_text:
                        full_thought_log += thought_text
                        thought_placeholder.markdown(full_thought_log)
                    elif part.text:
                        full_response += part.text
                        text_placeholder.markdown(full_response + "â–Œ")
                        
        text_placeholder.markdown(full_response)
        
        if synth_usage:
            total_usage["input"] += (synth_usage.prompt_token_count or 0)
            total_usage["output"] += (synth_usage.candidates_token_count or 0)
        
    except Exception as e:
        state_manager.add_debug_log(f"[Deep Research] Synthesis failed: {e}", "error")
        st.error(f"Synthesis failed: {e}")
        return "", None, None

    # å®Œäº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    thought_status.update(label="å¾¹åº•èª¿æŸ»å®Œäº† (Deep Research Finished)", state="complete", expanded=False)
    state_manager.add_debug_log("[Deep Research] Agent successfully finished.")

    # è¿”å´ç”¨ã«Usageã‚’æ•´å½¢
    final_usage_metadata = types.GenerateContentResponseUsageMetadata(
        prompt_token_count=total_usage["input"],
        candidates_token_count=total_usage["output"],
        total_token_count=total_usage["input"] + total_usage["output"]
    )

    # Queriesã®é‡è¤‡æ’é™¤
    combined_grounding["queries"] = list(set(combined_grounding["queries"]))

    return full_response, final_usage_metadata, combined_grounding